{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project 3\n",
    "\n",
    "## <em> Classification and inference with machine learning</em>\n",
    "<br>\n",
    "This notebook is arranged in cells. Texts are usually written in the markdown cells, and here you can use html tags (make it bold, italic, colored, etc). You can double click on this cell to see the formatting.<br>\n",
    "<br>\n",
    "The ellipsis (...) are provided where you are expected to write your solution but feel free to change the template (not over much) in case this style is not to your taste. <br>\n",
    "<br>\n",
    "<em>Hit \"Shift-Enter\" on a code cell to evaluate it.  Double click a Markdown cell to edit. </em><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Link Okpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from client.api.notebook import Notebook\n",
    "ok = Notebook('Project3_U.ok')\n",
    "_ = ok.auth(inline = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.integrate import quad\n",
    "#For plotting\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Problem 1 - Using Keras - MNIST\n",
    "\n",
    "The goal of this notebook is to introduce deep neural networks (DNNs) and convolutional neural networks (CNNs) using the high-level Keras package and to become familiar with how to choose its architecture, cost function, and optimizer in Keras. We will also learn how to train neural networks.\n",
    "\n",
    "We will once again work with the MNIST dataset of hand written digits introduced in HW8. The goal is to find a statistical model which recognizes and distinguishes between the ten handwritten digits (0-9).\n",
    "\n",
    "The MNIST dataset comprises handwritten digits, each of which comes in a square image, divided into a $28\\times 28$ pixel grid. Every pixel can take on $256$ nuances of the gray color, interpolating between white and black, and hence each data point assumes any value in the set $\\{0,1,\\dots,255\\}$. Since there are $10$ categories in the problem, corresponding to the ten digits, this problem represents a generic classification task. \n",
    "\n",
    "In this Notebook, we show how to use the Keras python package to tackle the MNIST problem with the help of deep neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating DNNs with Keras\n",
    "\n",
    "Constructing a Deep Neural Network to solve ML problems is a multiple-stage process. Quite generally, one can identify the key steps as follows:\n",
    "\n",
    "* ***step 1:*** Load and process the data\n",
    "* ***step 2:*** Define the model and its architecture\n",
    "* ***step 3:*** Choose the optimizer and the cost function\n",
    "* ***step 4:*** Train the model \n",
    "* ***step 5:*** Evaluate the model performance on the *unseen* test data\n",
    "* ***step 6:*** Modify the hyperparameters to optimize performance for the specific data set\n",
    "\n",
    "We would like to emphasize that, while it is always possible to view steps 1-5 as independent of the particular task we are trying to solve, it is only when they are put together in ***step 6*** that the real gain of using Deep Learning is revealed, compared to less sophisticated methods such as the regression models. With this remark in mind, we shall focus predominantly on steps 1-5 below. We show how one can use grid search methods to find optimal hyperparameters in ***step 6***."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Load and Process the Data\n",
    "\n",
    "Keras knows to download automatically the MNIST data from the web. All we need to do is import the `mnist` module and use the `load_data()` class, and it will create the training and test data sets or us.\n",
    "\n",
    "The MNIST set has pre-defined test and training sets, in order to facilitate the comparison of the performance of different models on the data.\n",
    "\n",
    "Once we have loaded the data, we need to format it in the correct shape ($({\\mathrm{N_{samples}}}, {\\mathrm{N_{features}}})$). \n",
    "\n",
    "The size of each sample, i.e. the number of bare features used is N_features (whis is 784 because we have a $28 \\times 28$ pixel grid), while the number of potential classification categories is \"num_classes\" (which is 10, number of digits).\n",
    "\n",
    "Each pixel\n",
    "contains a greyscale value quantified by an integer between\n",
    "0 and 255. To standardize the dataset, we normalize\n",
    "the input data in the interval [0, 1]. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import keras,sklearn\n",
    "# suppress tensorflow compilation warnings\n",
    "import os\n",
    "import tensorflow as tf\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "seed=0\n",
    "np.random.seed(seed) # fix random seed\n",
    "tf.set_random_seed(seed)\n",
    "\n",
    "from keras.datasets import mnist\n",
    "\n",
    "# input image dimensions\n",
    "num_classes = 10 # 10 digits\n",
    "\n",
    "img_rows, img_cols = 28, 28 # number of pixels \n",
    "\n",
    "# the data, shuffled and split between train and test sets\n",
    "(X_train, Y_train), (X_test, Y_test) = mnist.load_data()\n",
    "X_train = X_train[:40000]\n",
    "Y_train = Y_train[:40000]\n",
    "\n",
    "# reshape data, depending on Keras backend\n",
    "X_train = X_train.reshape(X_train.shape[0], img_rows*img_cols)\n",
    "X_test = X_test.reshape(X_test.shape[0], img_rows*img_cols)\n",
    "    \n",
    "# cast floats to single precesion\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "\n",
    "# rescale data in interval [0,1]\n",
    "X_train /= 255\n",
    "X_test /= 255\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\"> <i> 1. Make a plot of one MNIST digit (2D plot using X data - make sure to reshape it into a $28 \\times 28$ matrix) and label it (which digit does it correspond to?). </i></span> <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last, we cast the label vectors $y$ to binary class matrices (a.k.a. one-hot format)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert class vectors to binary class matrices\n",
    "\n",
    "print(\"before conversion - \")\n",
    "print(\"y vector : \", Y_train[0:10])\n",
    "\n",
    "Y_train = keras.utils.to_categorical(Y_train, num_classes)\n",
    "Y_test = keras.utils.to_categorical(Y_test, num_classes)\n",
    "\n",
    "print(\"after conversion - \")\n",
    "print(\"y vector : \", Y_train[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here in this template, we use 40000 training samples and 10000 test samples. Remember that we preprocessed data into the shape $({\\mathrm{N_{samples}}}, {\\mathrm{N_{features}}})$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('X_train shape:', X_train.shape)\n",
    "print('Y_train shape:', Y_train.shape)\n",
    "print()\n",
    "print(X_train.shape[0], 'train samples')\n",
    "print(X_test.shape[0], 'test samples')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Define the Neural Net and its Architecture\n",
    "\n",
    "We can now move on to construct our deep neural net. We shall use Keras's `Sequential()` class to instantiate a model, and will add different deep layers one by one.\n",
    "\n",
    "Let us create an instance of Keras' `Sequential()` class, called `model`. As the name suggests, this class allows us to build DNNs layer by layer. (https://keras.io/getting-started/sequential-model-guide/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "\n",
    "# instantiate model\n",
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the `add()` method to attach layers to our model. For the purposes of our introductory example, it suffices to focus on `Dense` layers for simplicity. (https://keras.io/layers/core/) Every `Dense()` layer accepts as its first required argument an integer which specifies the number of neurons. The type of activation function for the layer is defined using the `activation` optional argument, the input of which is the name of the activation function in `string` format. Examples include `relu`, `tanh`, `elu`, `sigmoid`, `softmax`.\n",
    "\n",
    "In order for our DNN to work properly, we have to make sure that the numbers of input and output neurons for each layer match. Therefore, we specify the shape of the input in the first layer of the model explicitly using the optional argument `input_shape=(N_features,)`. The sequential construction of the model then allows Keras to infer the correct input/output dimensions of all hidden layers automatically. Hence, we only need to specify the size of the softmax output layer to match the number of categories.\n",
    "\n",
    "First, add a `Dense` layer with 400 output neurons and `relu` activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Dense(400,input_shape=(img_rows*img_cols,), activation='relu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add another layer with 100 output neurons. Then, we will apply \"dropout,\" a regularization scheme that has been widely adopted in the neural networks literature: during the training procedure neurons\n",
    "are randomly “dropped out” of the neural network with some\n",
    "probability $p$ giving rise to a thinned network. It prevents overfitting by reducing spurious correlations between neurons within the network by introducing\n",
    "a randomization procedure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Dense(100, activation='relu'))\n",
    "# apply dropout with rate 0.5\n",
    "model.add(Dropout(0.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, we need to add a soft-max layer since we have a multi-class output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Dense(num_classes, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Choose the Optimizer and the Cost Function\n",
    "\n",
    "Next, we choose the loss function according to which to train the DNN. For classification problems, this is the cross entropy, and since the output data was cast in categorical form, we choose the `categorical_crossentropy` defined in Keras' `losses` module. Depending on the problem of interest one can pick any other suitable loss function. To optimize the weights of the net, we choose SGD. This algorithm is already available to use under Keras' `optimizers` module (https://keras.io/optimizers/), but we could use `Adam()` or any other built-in one as well. The parameters for the optimizer, such as `lr` (learning rate) or `momentum` are passed using the corresponding optional arguments of the `SGD()` function. \n",
    "\n",
    "While the loss function and the optimizer are essential for the training procedure, to test the performance of the model one may want to look at a particular `metric` of performance. For instance, in categorical tasks one typically looks at their `accuracy`, which is defined as the percentage of correctly classified data points. \n",
    "\n",
    "To complete the definition of our model, we use the `compile()` method, with optional arguments for the `optimizer`, `loss`, and the validation `metric` as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile the model\n",
    "model.compile(loss=keras.losses.categorical_crossentropy, optimizer='SGD', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Train the model\n",
    "\n",
    "We train our DNN in minibatches. Shuffling the training data during training improves stability of the model. Thus, we train over a number of training epochs. \n",
    "\n",
    "(The number of epochs is the number of complete passes through the training dataset, and the batch size is a number of samples propagated through the network before the model is updated.)\n",
    "\n",
    "Training the DNN is a one-liner using the `fit()` method of the `Sequential` class. The first two required arguments are the training input and output data. As optional arguments, we specify the mini-`batch_size`, the number of training `epochs`, and the test or validation data. To monitor the training procedure for every epoch, we set `verbose=True`. \n",
    "\n",
    "Let us set `batch_size` = 64 and `epochs` = 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training parameters\n",
    "batch_size = 64\n",
    "epochs = 10\n",
    "\n",
    "# train DNN and store training info in history\n",
    "history=model.fit(X_train, Y_train, batch_size=batch_size, epochs=epochs,\n",
    "          verbose=1, validation_data=(X_test, Y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Evaluate the Model Performance on the *Unseen* Test Data\n",
    "\n",
    "Next, we evaluate the model and read of the loss on the test data, and its accuracy using the `evaluate()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate model\n",
    "score = model.evaluate(X_test, Y_test, verbose=1)\n",
    "\n",
    "# print performance\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])\n",
    "\n",
    "# look into training history\n",
    "\n",
    "# summarize history for accuracy\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.ylabel('model accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='best')\n",
    "plt.show()\n",
    "\n",
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.ylabel('model loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Modify the Hyperparameters to Optimize Performance of the Model\n",
    "\n",
    "Last, we show how to use the grid search option of scikit-learn (https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html) to optimize the \n",
    "hyperparameters of our model.\n",
    "\n",
    "First, define a function for crating a DNN:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_DNN(optimizer=keras.optimizers.Adam()):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(400,input_shape=(img_rows*img_cols,), activation='relu'))\n",
    "    model.add(Dense(100, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=optimizer,\n",
    "              metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With epochs = 1 and batch_size = 64, do grid search over the following optimization schemes: ['SGD', 'RMSprop', 'Adagrad', 'Adadelta', 'Adam', 'Adamax', 'Nadam']."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "\n",
    "batch_size = 64\n",
    "epochs = 1\n",
    "model_gridsearch = KerasClassifier(build_fn=create_DNN, \n",
    "                        epochs=epochs, batch_size=batch_size, verbose=1)\n",
    "\n",
    "# list of allowed optional arguments for the optimizer, see `compile_model()`\n",
    "optimizer = ['SGD', 'RMSprop', 'Adagrad', 'Adadelta', 'Adam', 'Adamax', 'Nadam']\n",
    "# define parameter dictionary\n",
    "param_grid = dict(optimizer=optimizer)\n",
    "\n",
    "# call scikit grid search module\n",
    "grid = GridSearchCV(estimator=model_gridsearch, param_grid=param_grid, n_jobs=1, cv=4)\n",
    "grid_result = grid.fit(X_train,Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show the mean test score of all optimization schemes and determine which scheme gives the best accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\"> <i> 2. Create a DNN with one Dense layer having 200 output neurons. Do the grid search over any 5 different activation functions from https://keras.io/activations/. Suppose epochs = 1, batches = 64, p_dropout=0.5, and optimizer=keras.optimizers.Adam().  Make sure to print the mean test score of each case and determine which activation functions gives the best accuracy. </i></span> <br>\n",
    "\n",
    "<span style=\"color:red\"> <i> Doing the grid search requires quite a bit of memory. Please restart the kernel (\"Kernel\"-\"Restart\") and re-load the data before doing a new grid search. </i></span> <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\"> <i> 3. Now, do the grid search over different combination of batch sizes (10, 30, 50, 100) and number of epochs (1, 2, 5). Make sure to print the mean test score of each case and determine which activation functions gives the best accuracy. Here, you have a freedom to create your own DNN (assume an arbitrary number of Dense layers, optimization scheme, etc).  </i></span> <br>\n",
    "\n",
    "<span style=\"color:red\"> <i> Doing the grid search requires quite a bit of memory. Please restart the kernel (\"Kernel\"-\"Restart\") and re-load the data before doing a new grid search. </i></span> <br>\n",
    "\n",
    "Hint: To do the grid search over both batch_size and epochs, you can do:\n",
    "\n",
    "param_grid = dict(batch_size=batch_size, epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\"> <i> 4. Do the grid search over the number of neurons in the Dense layer and make a plot of mean test score as a function of num_neurons. Again, you have a freedom to create your own DNN. </i></span> <br>\n",
    "\n",
    "<span style=\"color:red\"> <i> Doing the grid search requires quite a bit of memory. Please restart the kernel (\"Kernel\"-\"Restart\") and re-load the data before doing a new grid search. </i></span> <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating CNNs with Keras\n",
    "\n",
    "<span style=\"color:red\"> <i> Please restart the kernel (\"Kernel\"-\"Restart\") and re-load the data. </i></span> <br>\n",
    "\n",
    "We have so far considered each MNIST data sample as a $(28\\times 28,)$-long 1d vector. This approach neglects any spatial structure in the image. On the other hand, we do know that in every one of the hand-written digits there are *local* spatial correlations between the pixels, which we would like to take advantage of to improve the accuracy of our classification model. To this end, we first need to reshape the training and test input data as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshape data, depending on Keras backend\n",
    "if keras.backend.image_data_format() == 'channels_first':\n",
    "    X_train = X_train.reshape(X_train.shape[0], 1, img_rows, img_cols)\n",
    "    X_test = X_test.reshape(X_test.shape[0], 1, img_rows, img_cols)\n",
    "    input_shape = (1, img_rows, img_cols)\n",
    "else:\n",
    "    X_train = X_train.reshape(X_train.shape[0], img_rows, img_cols, 1)\n",
    "    X_test = X_test.reshape(X_test.shape[0], img_rows, img_cols, 1)\n",
    "    input_shape = (img_rows, img_cols, 1)\n",
    "    \n",
    "print('X_train shape:', X_train.shape)\n",
    "print('Y_train shape:', Y_train.shape)\n",
    "print()\n",
    "print(X_train.shape[0], 'train samples')\n",
    "print(X_test.shape[0], 'test samples')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One can ask the question of whether a neural net can learn to recognize such local patterns. This can be achieved by using convolutional layers. Luckily, all we need to do is change the architecture of our DNN.\n",
    "\n",
    "![alt text](CNN.png \"Title\")\n",
    "\n",
    "After we instantiate the model, add the first convolutional layer with 10 filters, which is the dimensionality of output space. (https://keras.io/layers/convolutional/) Here, we will be concerned with local spatial filters\n",
    "that take as inputs a small spatial patch of the\n",
    "previous layer at all depths. We consider a three-dimensional kernel of size $5\\times5\\times1$. Check out this visualization of the\n",
    "convolution procedure for a square input of unit depth: https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md\n",
    "The convolution consists of running this filter over all locations\n",
    "in the spatial plane. After computing the filter, the output is passed through\n",
    "a non-linearity, a ReLU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(10, kernel_size=(5, 5),\n",
    "                 activation='relu',\n",
    "                 input_shape=input_shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Subsequently, add a 2D pooling layer. (https://keras.io/layers/pooling/) This pooling layer coarse-grain spatial information by performing\n",
    "a subsampling at each depth. Here, we use the the max pool operation. In a max pool, the spatial\n",
    "dimensions are coarse-grained by replacing a small region\n",
    "(say $2\\times2$ neurons) by a single neuron whose output is the\n",
    "maximum value of the output in the region."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(MaxPooling2D(pool_size=(2, 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add another convolutional layers with 20 filters and apply dropout. Then, add another pooling layer and flatten the data. You can do DNNs afterwards and compile the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add second convolutional layer with 20 filters\n",
    "model.add(Conv2D(20, (5, 5), activation='relu'))\n",
    "# apply dropout with rate 0.5\n",
    "model.add(Dropout(0.5))\n",
    "# add 2D pooling layer\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "# flatten data\n",
    "model.add(Flatten())\n",
    "# add a dense all-to-all relu layer\n",
    "model.add(Dense(20*4*4, activation='relu'))\n",
    "# apply dropout with rate 0.5\n",
    "model.add(Dropout(0.5))\n",
    "# soft-max layer\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "# compile the model\n",
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer='Adam',\n",
    "              metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, train your CNN and evaluate the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training parameters\n",
    "batch_size = 64\n",
    "epochs = 10\n",
    "\n",
    "\n",
    "# train CNN\n",
    "model.fit(X_train, Y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=1,\n",
    "          validation_data=(X_test, Y_test))\n",
    "\n",
    "# evaliate model\n",
    "score = model_CNN.evaluate(X_test, Y_test, verbose=1)\n",
    "\n",
    "# print performance\n",
    "print()\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\"> <i> 5. Do the grid search over any 3 different optimization schemes and 2 activation functions. Suppose that we have a 2 convolutional layers with 10 neurons. Let p_dropout = 0.5, epochs = 1, and batch_size = 64. Determine which combination of optimization scheme, activation function, and number of neurons gives the best accuracy. </i></span> <br>\n",
    "\n",
    "<span style=\"color:red\"> <i> Doing the grid search requires quite a bit of memory. Please restart the kernel (\"Kernel\"-\"Restart\") and re-load the data before doing a new grid search. </i></span> <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\"> <i> 6. Create an arbitrary DNN (you are free to choose any activation function, optimization scheme, etc) and evaluate its performance. Then, add two convolutional layers and pooling layers and evaluate its performance again. How do they compare? </i></span> <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Problem 2 - Using Tensorflow - Ising Model\n",
    "\n",
    "<span style=\"color:red\"> <i> You should restart the kernel for Problem 2. </i></span> <br>\n",
    "\n",
    "Next, we show how one can use deep neural nets to classify the states of the 2D Ising model according to their phase. This should be compared with the use of logistic-regression in HW8.\n",
    "\n",
    "The Hamiltonian for the classical Ising model is given by\n",
    "\n",
    "$$ H = -J\\sum_{\\langle ij\\rangle}S_{i}S_j,\\qquad \\qquad S_j\\in\\{\\pm 1\\} $$\n",
    "\n",
    "where the lattice site indices $i,j$ run over all nearest neighbors of a 2D square lattice, and $J$ is some arbitrary interaction energy scale. We adopt periodic boundary conditions. Onsager proved that this model undergoes a phase transition in the thermodynamic limit from an ordered ferromagnet with all spins aligned to a disordered phase at the critical temperature $T_c/J=2/\\log(1+\\sqrt{2})\\approx 2.26$. For any finite system size, this critical point is expanded to a critical region around $T_c$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Load and Process the Data\n",
    "\n",
    "We begin by writing a `DataSet` class and two functions `read_data_sets` and `load_data` to process the 2D Ising data. \n",
    "\n",
    "The `DataSet` class performs checks on the data shape and casts the data into the correct data type for the calculation. It contains a function method called `next_batch` which shuffles the data and returns a mini-batch of a pre-defined size. This structure is particularly useful for the training procedure in TensorFlow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "from __future__ import absolute_import, division, print_function\n",
    "import numpy as np\n",
    "seed=12\n",
    "np.random.seed(seed)\n",
    "import sys, os, argparse\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.framework import dtypes\n",
    "# suppress tflow compilation warnings\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "tf.set_random_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataSet(object):\n",
    "\n",
    "    def __init__(self,data_X,data_Y,dtype=dtypes.float32):\n",
    "        \"\"\"Checks data and casts it into correct data type. \"\"\"\n",
    "\n",
    "        dtype = dtypes.as_dtype(dtype).base_dtype\n",
    "        if dtype not in (dtypes.uint8, dtypes.float32):\n",
    "            raise TypeError('Invalid dtype %r, expected uint8 or float32' % dtype)\n",
    "\n",
    "        assert data_X.shape[0] == data_Y.shape[0], ('data_X.shape: %s data_Y.shape: %s' % (data_X.shape, data_Y.shape))\n",
    "        self.num_examples = data_X.shape[0]\n",
    "\n",
    "        if dtype == dtypes.float32:\n",
    "            data_X = data_X.astype(np.float32)\n",
    "        self.data_X = data_X\n",
    "        self.data_Y = data_Y \n",
    "\n",
    "        self.epochs_completed = 0\n",
    "        self.index_in_epoch = 0\n",
    "\n",
    "    def next_batch(self, batch_size, seed=None):\n",
    "        \"\"\"Return the next `batch_size` examples from this data set.\"\"\"\n",
    "\n",
    "        if seed:\n",
    "            np.random.seed(seed)\n",
    "\n",
    "        start = self.index_in_epoch\n",
    "        self.index_in_epoch += batch_size\n",
    "        if self.index_in_epoch > self.num_examples:\n",
    "            # Finished epoch\n",
    "            self.epochs_completed += 1\n",
    "            # Shuffle the data\n",
    "            perm = np.arange(self.num_examples)\n",
    "            np.random.shuffle(perm)\n",
    "            self.data_X = self.data_X[perm]\n",
    "            self.data_Y = self.data_Y[perm]\n",
    "            # Start next epoch\n",
    "            start = 0\n",
    "            self.index_in_epoch = batch_size\n",
    "            assert batch_size <= self.num_examples\n",
    "        end = self.index_in_epoch\n",
    "\n",
    "        return self.data_X[start:end], self.data_Y[start:end]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, load the Ising dataset, and splits it into three subsets: ordered, critical and disordered, depending on the temperature which sets the distribution they are drawn from. Once again, we use the ordered and disordered data to create a training and a test data set for the problem. Classifying the states in the critical region is expected to be harder and we only use this data to test the performance of our model in the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils import to_categorical\n",
    "import collections\n",
    "\n",
    "L=40 # linear system size\n",
    "\n",
    "# load data\n",
    "fac = 25\n",
    "file_name = \"Ising2DFM_reSample_L40_T=All.pkl\" # this file contains 16*10000 samples taken in T=np.arange(0.25,4.0001,0.25)\n",
    "data = pickle.load(open(file_name,'rb')) # pickle reads the file and returns the Python object (1D array, compressed bits)\n",
    "data = data[::fac]\n",
    "data = np.unpackbits(data).reshape(-1, 1600) # Decompress array and reshape for convenience\n",
    "data=data.astype('int')\n",
    "data[np.where(data==0)]=-1 # map 0 state to -1 (Ising variable can take values +/-1)\n",
    "\n",
    "file_name = \"Ising2DFM_reSample_L40_T=All_labels.pkl\" # this file contains 16*10000 samples taken in T=np.arange(0.25,4.0001,0.25)\n",
    "labels = pickle.load(open(file_name,'rb')) # pickle reads the file and returns the Python object (here just a 1D array with the binary labels)\n",
    "\n",
    "# divide data into ordered, critical and disordered\n",
    "X_ordered=data[:int(70000/fac),:]\n",
    "Y_ordered=labels[:70000][::fac]\n",
    "\n",
    "X_critical=data[int(70000/fac):int(100000/fac),:]\n",
    "Y_critical=labels[70000:100000][::fac]\n",
    "\n",
    "X_disordered=data[int(100000/fac):,:]\n",
    "Y_disordered=labels[100000:][::fac]\n",
    "\n",
    "del data,labels\n",
    "\n",
    "# define training and test data sets\n",
    "X=np.concatenate((X_ordered,X_disordered)) #np.concatenate((X_ordered,X_critical,X_disordered))\n",
    "Y=np.concatenate((Y_ordered,Y_disordered)) #np.concatenate((Y_ordered,Y_critical,Y_disordered))\n",
    "\n",
    "del X_ordered, X_disordered, Y_ordered, Y_disordered\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pick random data points from ordered and disordered states to create the training and test sets\n",
    "X_train,X_test,Y_train,Y_test=train_test_split(X,Y,train_size=0.6)\n",
    "\n",
    "\n",
    "# make data categorical\n",
    "Y_train=to_categorical(Y_train)\n",
    "Y_test=to_categorical(Y_test)\n",
    "Y_critical=to_categorical(Y_critical)\n",
    "\n",
    "\n",
    "# create data sets\n",
    "train = DataSet(X_train, Y_train, dtype=dtypes.float32)\n",
    "test = DataSet(X_test, Y_test, dtype=dtypes.float32)\n",
    "critical = DataSet(X_critical, Y_critical, dtype=dtypes.float32)\n",
    "\n",
    "Datasets = collections.namedtuple('Datasets', ['train', 'test', 'critical'])\n",
    "Dataset = Datasets(train=train, test=test, critical=critical)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can load the training data in the following way: (Dataset.train.data_X, Dataset.train.data_Y)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Steps 2+3: Define the Neural Net and its Architecture, Choose the Optimizer and the Cost Function\n",
    "\n",
    "We can now move on to construct our deep neural net using TensorFlow. \n",
    "\n",
    "Unique for TensorFlow is creating placeholders for the variables of the model, such as the feed-in data `X` and `Y` or the dropout probability `dropout_keepprob` (which has to be set to unity explicitly during testing). Another peculiarity is using the `with` scope to give names to the most important operators. While we do not discuss this here, TensorFlow also allows one to visualise the computational graph for the model (see package documentation on [https://www.tensorflow.org/](https://www.tensorflow.org/)).\n",
    "\n",
    "The shape of X is only partially defined. We know that it will be a matrix, with instances along the first dimension and features along the second dimension, and we know that the number of features is going to be $28\\times28$, but we don't know yet how many instances each training batch will contain. So the shape of X is (None, n_inputs). Similarly, we know that Y will be a vector with one entry per instance, but again we don't know the size of the training batch, so the shape is (None)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L=40 # system linear size\n",
    "n_feats=L**2 # 40x40 square lattice\n",
    "n_categories=2 # 2 Ising phases: ordered and disordered\n",
    "\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 100\n",
    "n_outputs = 2\n",
    "\n",
    "with tf.name_scope('data'):\n",
    "    X=tf.placeholder(tf.float32, shape=(None,n_feats))\n",
    "    Y=tf.placeholder(tf.float32, shape=(None,n_categories))\n",
    "    dropout_keepprob=tf.placeholder(tf.float32)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To classify whether a given spin configuration is in the ordered or disordered phase, we construct a minimalistic model for a DNN with a single hidden layer containing $N_\\mathrm{neurons}$ (which is kept variable so we can try out the performance of different sizes for the hidden layer). \n",
    "\n",
    "Let us use a neuron_layer() function to create layers in the neural nets.\n",
    "\n",
    "1. First, create a name scope using the name of the layer.\n",
    "2. Get the number of inputs by looking up the input matrix's shape and getting the size of the second dimension.\n",
    "3. Create a $W$ variable which holds the weight matrix (i.e. kernel).  Initialize it randomly, using a truncated normal distribution. \n",
    "4. Create a $b$ variable for biases, initialized to 0.\n",
    "5. Create a subgraph to compute $Z=XW+b$\n",
    "6. Use activation function if provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neuron_layer(X, n_neuron, name, activation = None):\n",
    "    with tf.name_scope(name):\n",
    "        n_inputs = int(X.get_shape()[1])\n",
    "        stddev = 2 / np.sqrt(n_inputs + n_neuron)\n",
    "        init = tf.truncated_normal((n_inputs, n_neuron), stddev = stddev)\n",
    "        W = tf.Variable(init, name = \"kernel\")\n",
    "        b = tf.Variable(tf.zeros([n_neuron]), name = \"bias\")\n",
    "        Z = tf.matmul(X, W) + b\n",
    "        if activation is not None:\n",
    "            return activation(Z)\n",
    "        else:\n",
    "            return Z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using a neuron_layer() function, create two hidden layers and an output layer. The first hidden layer takes X as its input, and the second takes the output of the first hidden layer as its input. Finally, the output layer takes the output of the second hidden layer as its input.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = tf.layers.dense(X, n_hidden1, activation = tf.nn.relu)\n",
    "    hidden2 = tf.layers.dense(hidden1, n_hidden2, activation = tf.nn.relu)\n",
    "    logits = tf.layers.dense(hidden2, n_outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, define the cost function that we will use to train the neural net model. Here, use the cross entropy to penalize models that estimate a low probability for the target class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope('loss'):\n",
    "    xentropy = tf.nn.softmax_cross_entropy_with_logits(labels = Y, logits = logits)\n",
    "    loss = tf.reduce_mean(xentropy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, define a GradientDescentOptimizer that will tweak the model parameters to minimize the cost function. Now, set learning_rate = 1e-6. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-6\n",
    "with tf.name_scope('optimiser'):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss) \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, specify how to evaluate the model. Let us simply use accuracy as our performance measure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with tf.name_scope('accuracy'):\n",
    "    correct_prediction = tf.equal(tf.argmax(Y, 1), tf.argmax(logits, 1))\n",
    "    correct_prediction = tf.cast(correct_prediction, tf.float64) # change data type\n",
    "#     correct_prediction = tf.nn.in_top_k(logits, Y, 1)\n",
    "    accuracy = tf.reduce_mean(correct_prediction)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Steps 4+5: Train the Model and Evaluate its Performance\n",
    "\n",
    "We train our DNN using mini-batches of size $100$ over a total of $100$ epochs, which we define first. We then set up the optimizer parameter dictionary `opt_params`, and use it to create a DNN model. \n",
    "\n",
    "Running TensorFlow requires opening up a `Session` which we abbreviate as `sess` for short. All operations are performed in this session by calling the `run` method. First, we initialize the global variables in TensorFlow's computational graph by running the `global_variables_initializer`. To train the DNN, we loop over the number of epochs. In each fix epoch, we use the `next_batch` function of the `DataSet` class we defined above to create a mini-batch. The forward and backward passes through the weights are performed by running the `loss` and `optimizer` methods. To pass the mini-batch as well as any other external parameters, we use the `feed_dict` dictionary. Similarly, we evaluate the model performance, by getting `accuracy` on the same minibatch data. Note that the dropout probability for testing is set to unity. \n",
    "\n",
    "Once we have exhausted all training epochs, we test the final performance on the entire training, test and critical data sets. This is done in the same way as above.\n",
    "\n",
    "Last, we return the loss and accuracy for each of the training, test and critical data sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_epochs=100\n",
    "batch_size=100\n",
    "\n",
    "with tf.Session() as sess:\n",
    "\n",
    "    # initialize the necessary variables, in this case, w and b\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    # train the DNN\n",
    "    for epoch in range(training_epochs): \n",
    "\n",
    "        batch_X, batch_Y = Dataset.train.next_batch(batch_size)\n",
    "\n",
    "        sess.run(optimizer, feed_dict={X: batch_X,Y: batch_Y,dropout_keepprob: 0.5})\n",
    "        \n",
    "\n",
    "    # test DNN performance on entire train test and critical data sets\n",
    "    train_loss, train_accuracy = sess.run([loss, accuracy], \n",
    "                                                feed_dict={X: Dataset.train.data_X, \n",
    "                                                           Y: Dataset.train.data_Y,\n",
    "                                                           dropout_keepprob: 0.5}\n",
    "                                                            )\n",
    "    print(\"train loss/accuracy:\", train_loss, train_accuracy)\n",
    "\n",
    "    test_loss, test_accuracy = sess.run([loss, accuracy], \n",
    "                                                feed_dict={X: Dataset.test.data_X,\n",
    "                                                           Y: Dataset.test.data_Y,\n",
    "                                                           dropout_keepprob: 1.0}\n",
    "                                                           )\n",
    "\n",
    "    print(\"test loss/accuracy:\", test_loss, test_accuracy)\n",
    "\n",
    "    critical_loss, critical_accuracy = sess.run([loss, accuracy], \n",
    "                                                feed_dict={X: Dataset.critical.data_X,\n",
    "                                                           Y: Dataset.critical.data_Y,\n",
    "                                                           dropout_keepprob: 1.0}\n",
    "                                                           )\n",
    "    print(\"crtitical loss/accuracy:\", critical_loss, critical_accuracy)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Modify the Hyperparameters to Optimize Performance of the Model\n",
    "\n",
    "To study the dependence of our DNN on some of the hyperparameters, we do a grid search over the number of neurons in the hidden layer, and different SGD learning rates. These searches are best done over logarithmically-spaced points. \n",
    "\n",
    "To do this, define a function for creating a DNN model: `create_DNN` and for evaluating the performance: `evaluate_model`.\n",
    "\n",
    "The function `grid_search` will output 2D heat map to show how accuracy changes with learning rate and number of neurons. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_DNN(n_hidden1=300, n_hidden2=100, learning_rate=1e-6):\n",
    "    with tf.name_scope('data'):\n",
    "        X=tf.placeholder(tf.float32, shape=(None,n_feats))\n",
    "        Y=tf.placeholder(tf.float32, shape=(None,n_categories))\n",
    "        dropout_keepprob=tf.placeholder(tf.float32)\n",
    "\n",
    "    with tf.name_scope(\"dnn\"):\n",
    "        hidden1 = tf.layers.dense(X, n_hidden1, activation = tf.nn.relu)\n",
    "        hidden2 = tf.layers.dense(hidden1, n_hidden2, activation = tf.nn.relu)\n",
    "        logits = tf.layers.dense(hidden2, n_outputs)\n",
    "        \n",
    "    with tf.name_scope('loss'):\n",
    "        xentropy = tf.nn.softmax_cross_entropy_with_logits(labels = Y, logits = logits)\n",
    "        loss = tf.reduce_mean(xentropy)\n",
    "        \n",
    "    with tf.name_scope('optimiser'):\n",
    "        optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss) \n",
    "\n",
    "    with tf.name_scope('accuracy'):\n",
    "        correct_prediction = tf.equal(tf.argmax(Y, 1), tf.argmax(logits, 1))\n",
    "        correct_prediction = tf.cast(correct_prediction, tf.float64) # change data type\n",
    "    #     correct_prediction = tf.nn.in_top_k(logits, Y, 1)\n",
    "        accuracy = tf.reduce_mean(correct_prediction)\n",
    "        \n",
    "    return X, Y, dropout_keepprob, loss, optimizer, accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(neurons,lr):\n",
    "    \n",
    "    training_epochs=100\n",
    "    batch_size=100\n",
    "\n",
    "\n",
    "    X, Y, dropout_keepprob, loss, optimizer, accuracy = create_DNN(n_hidden1=neurons, n_hidden2=neurons, learning_rate=lr)\n",
    "    with tf.Session() as sess:\n",
    "\n",
    "        # initialize the necessary variables, in this case, w and b\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        # train the DNN\n",
    "        for epoch in range(training_epochs): \n",
    "\n",
    "            batch_X, batch_Y = Dataset.train.next_batch(batch_size)\n",
    "\n",
    "            sess.run(optimizer, feed_dict={X: batch_X,Y: batch_Y,dropout_keepprob: 0.5})\n",
    "\n",
    "\n",
    "        # test DNN performance on entire train test and critical data sets\n",
    "        train_loss, train_accuracy = sess.run([loss, accuracy], \n",
    "                                                    feed_dict={X: Dataset.train.data_X, \n",
    "                                                               Y: Dataset.train.data_Y,\n",
    "                                                               dropout_keepprob: 0.5}\n",
    "                                                                )\n",
    "        print(\"train loss/accuracy:\", train_loss, train_accuracy)\n",
    "\n",
    "        test_loss, test_accuracy = sess.run([loss, accuracy], \n",
    "                                                    feed_dict={X: Dataset.test.data_X,\n",
    "                                                               Y: Dataset.test.data_Y,\n",
    "                                                               dropout_keepprob: 1.0}\n",
    "                                                               )\n",
    "\n",
    "        print(\"test loss/accuracy:\", test_loss, test_accuracy)\n",
    "\n",
    "        critical_loss, critical_accuracy = sess.run([loss, accuracy], \n",
    "                                                    feed_dict={X: Dataset.critical.data_X,\n",
    "                                                               Y: Dataset.critical.data_Y,\n",
    "                                                               dropout_keepprob: 1.0}\n",
    "                                                               )\n",
    "        print(\"crtitical loss/accuracy:\", critical_loss, critical_accuracy)\n",
    "\n",
    "    return train_loss,train_accuracy,test_loss,test_accuracy,critical_loss,critical_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_search():\n",
    "    \"\"\"This function performs a grid search over a set of different learning rates \n",
    "    and a number of hidden layer neurons.\"\"\"\n",
    "\n",
    "    # perform grid search over learnign rate and number of hidden neurons\n",
    "    N_neurons=[100, 200, 300, 400, 500]\n",
    "    learning_rates=np.logspace(-6,-1,6)\n",
    "\n",
    "    # pre-alocate variables to store accuracy and loss data\n",
    "    train_loss=np.zeros((len(N_neurons),len(learning_rates)),dtype=np.float64)\n",
    "    train_accuracy=np.zeros_like(train_loss)\n",
    "    test_loss=np.zeros_like(train_loss)\n",
    "    test_accuracy=np.zeros_like(train_loss)\n",
    "    critical_loss=np.zeros_like(train_loss)\n",
    "    critical_accuracy=np.zeros_like(train_loss)\n",
    "\n",
    "    # do grid search\n",
    "    for i, neurons in enumerate(N_neurons):\n",
    "        for j, lr in enumerate(learning_rates):\n",
    "\n",
    "            print(\"training DNN with %4d neurons and SGD lr=%0.6f.\" %(neurons,lr) )\n",
    "\n",
    "            train_loss[i,j],train_accuracy[i,j],\\\n",
    "            test_loss[i,j],test_accuracy[i,j],\\\n",
    "            critical_loss[i,j],critical_accuracy[i,j] = evaluate_model(neurons,lr)\n",
    "\n",
    "\n",
    "    plot_data(learning_rates,N_neurons,train_accuracy, \"training data\")\n",
    "    plot_data(learning_rates,N_neurons,test_accuracy, \"test data\")\n",
    "    plot_data(learning_rates,N_neurons,critical_accuracy, \"critical data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_data(x,y,data, title):\n",
    "\n",
    "    # plot results\n",
    "    fontsize=16\n",
    "\n",
    "\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    cax = ax.matshow(data, interpolation='nearest', vmin=0, vmax=1)\n",
    "    fig.colorbar(cax)\n",
    "\n",
    "    # put text on matrix elements\n",
    "    for i, x_val in enumerate(np.arange(len(x))):\n",
    "        for j, y_val in enumerate(np.arange(len(y))):\n",
    "            c = \"${0:.1f}\\\\%$\".format( 100*data[j,i])  \n",
    "            ax.text(x_val, y_val, c, va='center', ha='center')\n",
    "\n",
    "    # convert axis vaues to to string labels\n",
    "    x=[str(i) for i in x]\n",
    "    y=[str(i) for i in y]\n",
    "\n",
    "\n",
    "    ax.set_xticklabels(['']+x)\n",
    "    ax.set_yticklabels(['']+y)\n",
    "\n",
    "    ax.set_xlabel('$\\\\mathrm{learning\\\\ rate}$',fontsize=fontsize)\n",
    "    ax.set_ylabel('$\\\\mathrm{hidden\\\\ neurons}$',fontsize=fontsize)\n",
    "    \n",
    "    ax.set_title(title,fontsize=fontsize)\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "grid_search()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\"> <i> 1. Do the grid search over 5 different types of activation functions (https://www.tensorflow.org/api_guides/python/nn#Activation_Functions). Evaluate the performance for each case and determine which gives the best accuracy. You can assume an arbitrary DNN. Show results for training, test, and critical data. </i></span> <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\"> <i> 2. Do the grid search over 5 different numbers of epochs and batch sizes. Make a 2D heat map as shown in the example. You can assume an arbitrary DNN. Show results for training, test, and critical data.  </i></span> <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Problem 3 - SDSS galaxies\n",
    "\n",
    "\n",
    "<span style=\"color:red\"> <i> You should restart the kernel for Problem 2. </i></span> <br>\n",
    "\n",
    "\n",
    "The data is provided in the file <b>\"specz_data.txt\"</b>. The columns of the file (length of 13) correspond to - <br>\n",
    "spectroscopic redshift ('zspec'), RA, DEC, magnitudes in 5 bands - u, g, r, i, z (denoted as 'mu,' 'mg,' 'mr,' 'mi,' 'mz' respectively); Exponential and de Vaucouleurs model magnitude fits ('logExp' and 'logDev' http://www.sdss.org/dr12/algorithms/magnitudes/); zebra fit ('pz_zebra); Neural Network fit ('pz_NN') and its error estimate ('pz_NN_Err') <br>\n",
    "\n",
    "We will undertake 2 exercises  - \n",
    "- Regression\n",
    "    - We will use the magnitude of object in different bands ('mu, mg, mr, mi, mz') and do a regression exercise to estimate the redshift of the object. Hence our feature space is 5.\n",
    "    - The correct redshift is given by 'zspec', which is the spectroscopic redshift of the object. We will use this for training and testing purpose. \n",
    "    \n",
    "    Sidenote: Photometry vs. Spectroscopy\n",
    "    \n",
    "    <i>&nbsp; &nbsp; The amount of energy we receive from celestial objects – in the form of radiation – is called the flux, and an astro- nomical technique of measuring the flux is photometry. Flux is usually measured over broad wavelength bands, and with the estimate of the distance to an object, it can infer the object’s luminosity, temperature, size, etc. Usually light is passed through colored filters, and we measure the intensity of the filtered light. \n",
    "    \n",
    "    &nbsp; &nbsp; On the other hand, spectroscopy deals with the spectrum of the emitted light. This tells us what the object is made of, how it is moving, the pressure of the material in it, etc. Note that for faint objects making photometric observation is much easier.\n",
    "    \n",
    "    &nbsp; &nbsp; Photometric redshift (photoz) is an estimate of the distance to the object using photometry. Spectroscopic redshift observes the object’s spectral lines and measures their shifts due to the Doppler effect to infer the distance.</i>\n",
    "    \n",
    "\n",
    "- Classification\n",
    "    - We will use the same magnitudes and now also the redshift of the object  ('zspec') to classify the object as either Elleptical or Spiral. Hence our feature space is now 6.\n",
    "    - The correct class is given by compring 'logExp' and 'logDev' which are the fits for Exponential and Devocular profiles. If logExp > logDev, its a spiral and vice-versa. We will use this for training and testing purpose. Since the classes are not explicitly given, generate a column for those (Classes can be $\\pm 1$. If it is $0$, it does not belong to either of the class.)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Cleaning\n",
    "\n",
    "Read in the files to create the data (X and Y) for both regression and classification. <br>\n",
    "You will have to clean the data - \n",
    "- Drop the entries that are nan or infinite\n",
    "- Drop the unrealistic numbers such as 999, -999; and magnitudes that are unrealistic. Since these are absolute magnitudes, they should be positive and high. Lets choose a magnitude limit of 15 as safe bet.\n",
    "- For classification, drop the entries that do not belong to either of the class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read in and create data\n",
    "\n",
    "fname = 'specz_data.txt'\n",
    "spec_dat=np.genfromtxt(fname,names=True)\n",
    "print(spec_dat.dtype.fields.keys())\n",
    "#convenience variable\n",
    "zspec = spec_dat['zspec']\n",
    "pzNN = spec_dat['pz_NN']\n",
    "#some N redshifts are not defined\n",
    "pzNN[pzNN < 0] = np.nan\n",
    "\n",
    "#For Regression\n",
    "bands = ['u', 'g', 'r','i', 'z' ]\n",
    "mlim = 15\n",
    "\n",
    "xdata = np.concatenate([[spec_dat['m%s'%i] for i in bands]]).T\n",
    "bad = (xdata[:, 0] < mlim) | (xdata[:, 1] < mlim) | (xdata[:, 2] < mlim) & (xdata[:, 3] < mlim) | (xdata[:, 4] < mlim)\n",
    "xdata = xdata[~bad]\n",
    "xdata[xdata<0] = 0\n",
    "ydata = zspec[~bad]\n",
    "\n",
    "#For classification\n",
    "classes = np.sign(spec_dat['logExp'] - spec_dat['logDev'])\n",
    "tmp = np.concatenate([[spec_dat['m%s'%i] for i in bands]]).T\n",
    "xxdata = np.concatenate([tmp, zspec.reshape(-1, 1)], axis=1)\n",
    "bad = (classes==0) | (xxdata[:, 0] < mlim) | (xxdata[:, 1] < mlim) | (xxdata[:, 2] < mlim) & (xxdata[:, 3] < mlim) | (xxdata[:, 4] < mlim)\n",
    "xxdata = xxdata[~bad]\n",
    "classes = classes[~bad]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For regression, X and Y data (called \"xdata\" and \"ydata,\" respectively) is cleaned magnitudes (5 feature space) and spectroscopic redshifts respectively.\n",
    "For classification, X and Y data (called \"xxdata\" and \"classes\" respectively) is cleaned magnitudes+spectroscopic redshifts respectively (6 feature space) and classees respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('For Regression:')\n",
    "print('Before: Size of datasets is ', zspec.shape[0])\n",
    "print('After: Size of datasets is ', xdata.shape[0])\n",
    "print('')\n",
    "print('For Classification:')\n",
    "print('Before: Size of datasets is ', zspec.shape[0])\n",
    "print('After: Size of datasets is ', xxdata.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Visualization\n",
    "\n",
    "The next step should be to visualize the data. <br>\n",
    "For regression\n",
    "- Make a histogram for the distribution of the data (spectroscopic redshift). \n",
    "- Make 5 2D histograms of the distribution of the magnitude as function of redshift (Hint: https://matplotlib.org/devdocs/api/_as_gen/matplotlib.axes.Axes.hist2d.html)\n",
    "\n",
    "For classification <br>\n",
    "- Make 6 1-d histogram for the distribution of the data (6 features - zspec and 5 magnitudes) for both class 1 and -1 separately \n",
    "\n",
    "<span style=\"color:blue\"> <i> 1. Make histograms for both regression and classification. </i></span> <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\"> <i> 2. Do the following preprocessing: </i></span> <br>\n",
    "\n",
    "#####  Preprocessing:\n",
    "\n",
    "- Next, split the sample into training data and the testing data. We will be using the training data to train different algorithms and then compare the performance over the testing data. In this project, keep 80% data as training data and uses the remaining 20% data for testing.  <br>\n",
    "- Often, the data can be ordered in a specific manner, hence shuffle the data prior to splitting it into training and testing samples. <br>\n",
    "- Many algorithms are also not scale invariant, and hence scale the data (different features to a uniform scale). All this comes under preprocessing the data.\n",
    "http://scikit-learn.org/stable/modules/preprocessing.html#preprocessing <br>\n",
    "Use StandardScaler from sklearn (or write your own routine) to center the data to 0 mean and 1 variance. Note that you only center the training data and then use its mean and variance to scale the testing data before using it. <br><br>\n",
    "\n",
    "Hint: How to get a scaled training data: <br>\n",
    "\n",
    "1. Let the training data be: train = (\"training X data\", \"training Y data\")<br>\n",
    "2. You can first define a StandardScaler: <br>\n",
    "scale_xdata, scale_ydata = preprocessing.StandardScaler(), preprocessing.StandardScaler()<br>\n",
    "3. Then, do the fit: <br>\n",
    "for regression: scale_xdata.fit(train_regression[0]), scale_ydata.fit(train_regression[1].reshape(-1, 1))<br>\n",
    "for classication: scale_xdata.fit(train_classification[0])<br>\n",
    "  Here, no need to fit for y data for classification (it's either +1 or -1. Already scaled)<br>\n",
    "4. Next, transform: <br>\n",
    " for regression: scaled_train_data = (scale_xdata.fit_transform(train_regression[0]), scale_ydata.fit_transform(train_regression[1].reshape(-1, 1)))<br>\n",
    " for classication: scaled_train_data = (scale_xdata.fit_transform(train_classification[0]), train_classification[1])<br>\n",
    " Again, y data is already scaled for classification.  <br>\n",
    "\n",
    "\n",
    "Do this for test data as well.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Metrics\n",
    "\n",
    "The last remaining preperatory step is to write metric for gauging the performance of the algorithm. Write a function to calculate the 'RMS' error given (y_predict, y_truth) to gauge regression and another function to evaluate accuracy of classification. <br>\n",
    "In addition, for classification, we will also use confusion matrix.\n",
    "\n",
    "Below is an example you can use. Feel free to write you own."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def rms(x, y, scale1=None, scale2=None):\n",
    "    '''Calculate the rms error given the truth and the prediction\n",
    "    '''\n",
    "    mask = np.isfinite(x[:]) & np.isfinite(y[:])\n",
    "    if scale1 is not None:\n",
    "        x= scale1.inverse_transform(x)\n",
    "    if scale2 is not None:\n",
    "        y = scale2.inverse_transform(y)\n",
    "    return  np.sqrt(np.mean((x[mask] - y[mask]) ** 2))\n",
    "\n",
    "def acc(x, y):\n",
    "    '''Calculate the accuracy given the truth and the prediction\n",
    "    '''\n",
    "    mask = np.isfinite(x[:]) & np.isfinite(y[:])\n",
    "    return  (x == y).sum()/x.size\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameter method\n",
    "\n",
    "Now, we will be varying hyperparameters to get the best model and build some intuition. There are various ways to do this and we will use Grid Search methodology (as you did in Problem 1 and 2) which simply tries all the combinations along with some cross-validation scheme. For most part, we will use 4-fold cross validation. <br>\n",
    "Sklearn provides GridSearchCV functionality for this purpose. <br>\n",
    "\n",
    "Its recommended to spend some time to go through output format of GridSearchCV and write some utility functions to make the recurring plots for every parameter. <br>\n",
    "Grid Search returns a dictionary with self explanatory keys for the most part. Mostly, the keys correspond to (masked) numpy arrays of size = #(all possible combination of parameters). The value of individual parameter in every combination is given in arrays with keys starting from 'param_\\*' and this should help you to match the combination with the corresponding scores. <br>\n",
    "For masked arrays, you can access the data values by using \\*.data\n",
    "<br>\n",
    "<br>\n",
    "*Do not overwrite these grid search-ed variables (and not only their result) since we will compare all the models together in the end*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "# http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html\n",
    "# http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 1. k Nearest Neighbors\n",
    "\n",
    "For regression, let us play with grid search using knn to tune hyperparmeters. (https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsRegressor.html) Consider the following 3 hyperparameters - \n",
    "- Number of neighbors ([2, 3, 5, 10, 15, 20, 25, 50, 100])\n",
    "- Weights of leaves (Uniform or Inverse Distance weighing)\n",
    "- Distance metric (Eucledian or Manhattan distance - parameter 'p')\n",
    "\n",
    "<span style=\"color:blue\"> <i> 1. Do a grid search on these parameters. List the combination of hyperparameters you tried and evaluate the accuracy (mean test score) and its standard deviation. Which gives the highest accuracy value? </i></span> <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hint: (Read the documentations carefully for more detail.)\n",
    "\n",
    "First, define the hyperparameters: parameters = {'n_neighbors':[2, 3, 5, 10, 15, 20, 25, 50, 100], 'weights':['uniform', 'distance'], 'p':[1, 2]}\n",
    "\n",
    "Specify the algorithm you want to use: e.g. knnr = KNeighborsRegressor() \n",
    "\n",
    "Then, Do a grid search on these parameters using 4 fold cross validation: gcknn = GridSearchCV(knnr, parameters, cv=4)\n",
    "\n",
    "Do the fit: gcknn.fit(*scaled_training_data) \n",
    "\n",
    "(Let \"scaled_training_data\" be the training data where \"scaled_training_data = (\"train X data\", \"train Y data\")\"\n",
    "\n",
    "Get results: $results = gcknn.cv_results_$\n",
    "\n",
    "$cv_results_$ has the following dictionaries: \"rank_test_score,\" \"mean_test_score,\" \"std_test_score,\" and \"params\" (See http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html) \n",
    "\n",
    "Then, you can evaluate the models based on \"rank_test_score\" and print out their \"params,\" along with their \"mean_test_score\" and \"std_test_score\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\"> <i> 2. Also print out fitting and scoring times for all hyperparameter combinations. </i></span> <br>\n",
    "\n",
    "*Plot timings for fitting and scoring*\n",
    "\n",
    "Hint: Assume that you got results from: $results = gcknn.cv_results_$\n",
    "\n",
    "Then, get the scoring time: results['mean_score_time']\n",
    "\n",
    "and the fitting time: results['mean_fit_time']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\"> <i> 3. Based on the results you obtained in Part 1 and 2, answer the following questions </i></span> <br>\n",
    "\n",
    "- Is it always better to use more neighbors?\n",
    "- Is it better to weigh the leaves, if yes, which distnace metric performs better?\n",
    "- GridCV returns fitting and scoring time for every combination. You will find that scoring time is higher than training time. Why do you think is that the case?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Answer:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\"> <i> 4. Which parameters seem to affect the performance most? To better answer this question, make plots of the mean test score for each hyperparameter. </i></span> <br>\n",
    "\n",
    "Hint:\n",
    "Suppose you have two types of hyperparameters: A and B.\n",
    "Let A = [1, 2] and B = [1, 2, 4, 7, 10].\n",
    "\n",
    "Then, you have 20 different combination of hyperparameters.\n",
    "\n",
    "Let A = 1. Then, you can try (A,B) = (1,1), (1,2), (1,4), (1,7), (1,10)\n",
    "Suppose that the mean score you got for the above combination is [0.7, 0.72, 0.75, 0.77, 0.8].\n",
    "Similarly, for A = 2, you tried (A,B) = (2,1), (2,2), (2,4), (2,7), (2,10) and obtaind the mean score of [0.8, 0.82, 0.85, 0.87, 0.9].\n",
    "\n",
    "To better see how changing the value of paramter A affects the performance, you can make the following plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A_1 = [0.7, 0.72, 0.75, 0.77, 0.8]\n",
    "A_2 = [0.8, 0.82, 0.85, 0.87, 0.9]\n",
    "\n",
    "plt.plot(A_1, label = \"A=1\")\n",
    "plt.plot(A_2, label = \"A=2\")\n",
    "plt.ylabel(\"mean test score\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the plot of the mean test score for A marginalizing over B.\n",
    "\n",
    "Similarly, make a plot of the mean test score for each kNN hyperparameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\"> <i> 5. You have determined the best combination of hyperparameters and CV schemes. Predict the test y data using the GridSearchCV method. Use the \"rms\" metric function we defined earlier and calculate the rms error on the test data.  </i></span> <br>\n",
    "\n",
    "Hint: To determine the rms error, you need:\n",
    "\n",
    "Truth: given from data (test_data[1]) <br>\n",
    "Prediction: gridsearch.predict(test_data[0])\n",
    "(https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "# http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will look at 4 different type of cross-validation schemes - \n",
    "- Kfold\n",
    "- Stratified Kfold\n",
    "- Shuffle Split\n",
    "- Stratified Shuffle Split\n",
    "\n",
    "<span style=\"color:blue\"> <i> 6. Assuming the list of hyperparameters from Part 1, do 4 different grid searches. From Part 1, take top 5 combination of hyperparameters which gives you the highest accuracy value. Rank the performance of CV schemes for each combination.  </i></span> <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold, StratifiedKFold, ShuffleSplit, StratifiedShuffleSplit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {'n_neighbors':[2, 3, 5, 10, 15, 20, 25, 50, 100], 'weights':['uniform', 'distance'], 'p':[1, 2]}\n",
    "knnc = KNeighborsClassifier()\n",
    "\n",
    "#Grid Search\n",
    "gc = GridSearchCV(knnc, parameters, cv=KFold(4, random_state=100))\n",
    "#Do the fit\n",
    "...\n",
    "\n",
    "gc2 = GridSearchCV(knnc, parameters, cv=StratifiedKFold(4, random_state = 100))\n",
    "#Do the fit\n",
    "...\n",
    "\n",
    "gc3 = GridSearchCV(knnc, parameters, cv=ShuffleSplit(4, 0.1, random_state = 100))\n",
    "#Do the fit\n",
    "...\n",
    "\n",
    "gc4 = GridSearchCV(knnc, parameters, cv=StratifiedShuffleSplit(4, 0.1, random_state = 100))\n",
    "#Do the fit\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\"> <i> 7. Answer the following questions: </i></span> <br>\n",
    "\n",
    "- Are the conclusions different for any parameter from the regression case?\n",
    "- Does the mean accuracy change for different CV scheme?\n",
    "- Does the standard deviation in mean accuracy change?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Answer:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\"> <i> 8. Using the best combination of hyperparameters and CV schemes you have found, compute the confusion matrix (https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html) and evaluate the accuracy.  </i></span> <br>\n",
    "\n",
    "Hint: To get a confusion matrix, you need both truth (available from data) and prediction (can be computed using .predict function from GridSearchCV (https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 2. Random Forests\n",
    "\n",
    "The most important feature of the random forest is the number of trees in the ensemble. We will also play with the maximum depth of the trees.\n",
    "\n",
    "Try:<br>\n",
    "n_estimators = [10, 50, 150, 200, 300]<br>\n",
    "max_depth = [10, 50, 100]<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "# http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\"> <i> 1. Do the grid search over n_estimators and max_depth. List the combination of hyperparameters you tried and evaluate the accuracy (mean test score) and its standard deviation. Which gives the highest accuracy value?  </i></span> <br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestRegressor()\n",
    "parameters = ...\n",
    "\n",
    "gcrf = GridSearchCV(rf, parameters, cv=5)\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\"> <i> 2. Which parameters seem to affect the performance most? To better answer this question, make plots of the mean test score for each hyperparameter. (plot the mean test score of n_estimators marginalizing over max_depth, etc) </i></span> <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\"> <i> 3. Based on the results you obtained in Part 1, answer the following questions: </i></span> <br>\n",
    "\n",
    "- Are the scores of these models statistically different? Based on this, which architecture will you choose for your model?\n",
    "- For every parameter, make the plot for fitting time. Based on this and the previous question, how many trees do you recommend keeping in the ensemble?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Answer:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\"> <i> 4. You have determined the best combination of hyperparameters. Predict the test y data using the GridSearchCV method. Use the \"rms\" metric function we defined earlier and calculate the rms error on the test data.  </i></span> <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classification\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "# http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Grid search (This will take few minutes)\n",
    "\n",
    "rfc = RandomForestClassifier()\n",
    "parameters = ...\n",
    "\n",
    "gcrfc = GridSearchCV(rfc, parameters, cv=StratifiedShuffleSplit(4, 0.1, random_state = 100))\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\"> <i> 5. Assuming the list of hyperparameters from Part 1, do the grid search using StratifiedShuffleSplit CV scheme. List the combination of hyperparameters you tried and evaluate the accuracy (mean test score) and its standard deviation. Which gives the highest accuracy value?  </i></span> <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\"> <i> 6. Using the best combination of hyperparameters, compute the confusion matrix (https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html) and evaluate the accuracy.  </i></span> <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To Submit\n",
    "Execute the following cell to submit.\n",
    "If you make changes, execute the cell again to resubmit the final copy of the notebook, they do not get updated automatically.<br>\n",
    "__We recommend that all the above cells should be executed (their output visible) in the notebook at the time of submission.__ <br>\n",
    "Only the final submission before the deadline will be graded. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = ok.submit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
