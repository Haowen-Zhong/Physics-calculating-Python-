{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homework 4\n",
    "\n",
    "## <em> Linear Algebra and Data Modeling</em>\n",
    "<br>\n",
    "This notebook is arranged in cells. Texts are usually written in the markdown cells, and here you can use html tags (make it bold, italic, colored, etc). You can double click on this cell to see the formatting.<br>\n",
    "<br>\n",
    "The ellipsis (...) are provided where you are expected to write your solution but feel free to change the template (not over much) in case this style is not to your taste. <br>\n",
    "<br>\n",
    "<em>Hit \"Shift-Enter\" on a code cell to evaluate it.  Double click a Markdown cell to edit. </em><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Link Okpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from client.api.notebook import Notebook\n",
    "ok = Notebook('hw4_U.ok')\n",
    "_ = ok.auth(inline = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.integrate import quad\n",
    "import sklearn as sk\n",
    "from sklearn import datasets, linear_model\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "#For plotting\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Problem 1 - Solving Least Squares Using Normal Equations and SVD\n",
    "\n",
    "(Reference - NR 15.4) We fit a set of 50 data points $(x_i, y_i)$ to a polynomial $y(x) = a_0 + a_1x + a_2x^2 + a_3x^3$. (Note that this problem is linear in $a_i$ but nonlinear in $x_i$). The uncertainty $\\sigma_i$ associated with each measurement $y_i$ is known, and we assume that the $x_i$'s are known exactly. To measure how well the model agrees with the data, we use the chi-square merit function: <br>\n",
    "$$ \\chi^2 = \\sum_{i=0}^{N-1} \\big( \\frac{y_i-\\sum_{k=0}^{M-1}a_k x^k}{\\sigma_i} \\big)^2. $$\n",
    "<br>\n",
    "where N = 50 and M = 4. Here, $1, x, ... , x^3$ are the basis functions.\n",
    "<br><br>\n",
    "<span style=\"color:blue\"> <i> 1. Plot data (make sure to include error bars). (Hint - https://matplotlib.org/api/_as_gen/matplotlib.axes.Axes.errorbar.html) </i></span><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load a given 2D data\n",
    "data = np.loadtxt(\"HW4_Problem2_data.dat\")\n",
    "x = data[:,0]\n",
    "y = data[:,1]\n",
    "sig_y = data[:,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Make plot\n",
    "plt.figure(figsize = (10, 7))\n",
    "# Scatter plot\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will pick as best parameters those that minimize $\\chi^2$.<br><br>\n",
    "First, let $\\bf A$ be a matrix whose $N \\times M$ components are constructed from the $M$ basis functions evaluated at the $N$ abscissas $x_i$, and from the $N$ measurement errors $\\sigma_i$, by the prescription\n",
    "$$ A_{ij} = \\frac{X_j(x_i)}{\\sigma_i} $$\n",
    "<br>where $X_0(x) = 1,\\ X_1(x) = x,\\ X_2(x) = x^2,\\ X_3(x) = x^3$. We call this matrix $\\bf A$ the design matrix.\n",
    "<br><br>\n",
    "Also, define a vector $\\bf b$ of length $N$ by\n",
    "$$ b_i = \\frac{y_i}{\\sigma_i} $$\n",
    "<br>and denote the $M$ vector whose components are the parameters to be fitted ($a_0, a_1, a_2, a_3$) by $\\bf a$.\n",
    "<br><br>\n",
    "<span style=\"color:blue\"><i> 2. Define the design matrix A. (Hint: Its dimension should be NxM = 50x4.) Also, define the vector b. </i></span><br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define A\n",
    "A = ...\n",
    "# Define b\n",
    "b = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Minimize $\\chi^2$ by differentiating it with respect to all $M$ parameters $a_k$ vaishes. This condition yields the matrix equation <br>\n",
    "$$ \\sum_{j=0}^{M-1} \\alpha_{kj}a_j = \\beta_k$$ \n",
    "<br> where $\\bf \\boldsymbol \\alpha = A^T \\cdot A$ and $\\bf \\boldsymbol \\beta = A^T \\cdot b$ ($\\boldsymbol \\alpha$ is an $M \\times M$ matrix, and $\\boldsymbol \\beta$ is a vector of length $M$). This is the normal equation of the least squares problem. In matrix form, the normal equations can be written as:\n",
    "$$ \\bf \\boldsymbol \\alpha \\cdot a = \\boldsymbol \\beta. $$\n",
    "<br><br>\n",
    "This can be solved for the vector of parameters $\\bf a$ by linear algebra numerical methods.\n",
    "<br><br>\n",
    "<span style=\"color:blue\"> <i> 3. Define the matrix alpha and vector beta. </i></span><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Transpose of the matrix A\n",
    "A_transpose = ...\n",
    "\n",
    "# alpha matrix\n",
    "alpha = ...\n",
    "# beta vector\n",
    "beta = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\"> <i> 4. We have $ \\bf \\boldsymbol \\alpha \\cdot a = \\boldsymbol \\beta. $ Solve for $\\bf a$ using (1) \"GaussianElimination_pivot\" defined below (2) LU decomposition and forward subsitution and backsubstitution. Plot the best-fit line on top of the data. </i></span><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GaussianElimination_pivot(A, b):\n",
    "    \n",
    "    N = len(b)\n",
    "\n",
    "    for m in range(N):\n",
    "        \n",
    "        # Check if A[m,m] is the largest value from elements bellow and perform swapping\n",
    "        for i in range(m+1,N):\n",
    "            if A[m,m] < A[i,m]:\n",
    "                A[[m,i],:] = A[[i,m],:]\n",
    "                b[[m,i]] = b[[i,m]]\n",
    "    \n",
    "        # Divide by the diagonal element\n",
    "        div = A[m,m]\n",
    "        A[m,:] /= div\n",
    "        b[m] /= div\n",
    "\n",
    "        # Now subtract from the lower rows\n",
    "        for i in range(m+1,N):\n",
    "            mult = A[i,m]\n",
    "            A[i,:] -= mult*A[m,:]\n",
    "            b[i] -= mult*b[m]\n",
    "\n",
    "    # Backsubstitution\n",
    "    x = np.empty(N,float)\n",
    "    for m in range(N-1,-1,-1):\n",
    "        x[m] = b[m]\n",
    "        for i in range(m+1,N):\n",
    "            x[m] -= A[m,i]*x[i]\n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Using the Gaussian elimination with partial pivoting\n",
    "a = ...\n",
    "\n",
    "print('Using Gaussian Elimination:')\n",
    "print('a0 =', a[0], ', a1 =', a[1], ', a2 =', a[2], ', a3 =', a[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# \"lu\" does LU decomposition with pivot. Reference - https://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.linalg.lu_factor.html\n",
    "from scipy.linalg import lu\n",
    "\n",
    "def solve_lu_pivot(A, B):\n",
    "    # LU decomposition with pivot\n",
    "    L, U = lu(A, permute_l=True)\n",
    "    \n",
    "    N = len(B)\n",
    "    \n",
    "    # forward substitution: We have Ly = B. Solve for y\n",
    "    ...\n",
    "\n",
    "    # backward substitution: We have y = Ux. Solve for x.\n",
    "    ...\n",
    "    \n",
    "    return ...\n",
    "\n",
    "a = ...\n",
    "\n",
    "print('Using LU Decomposition:')\n",
    "print('a0 =', a[0], ', a1 =', a[1], ', a2 =', a[2], ', a3 =', a[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Make plot\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The inverse matrix $\\bf C = \\boldsymbol \\alpha^{-1}$ is called the covariance matrix, which is closely related to the probable uncertainties of the estimated parameters $\\bf a$. To estimate these uncertainties, we compute the variance associated with the estimate $a_j$. Following NR p.790, we obtain: <br><br>\n",
    "$$ \\sigma^2(a_j) = \\sum_{k=0}^{M-1} \\sum_{l=0}^{M-1} C_{jk} C_{jl} \\alpha_{kl} = C_{jj} $$\n",
    "<br>\n",
    "<span style=\"color:blue\"> <i> 5. Compute the error (standard deviation - square root of the variance) on the fitted parameters. </i></span><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.linalg import inv\n",
    "# Covariance matrix\n",
    "\n",
    "...\n",
    "\n",
    "sigma_a0 = ...\n",
    "sigma_a1 = ...\n",
    "sigma_a2 = ...\n",
    "sigma_a3 = ...\n",
    "\n",
    "print('Error: on a0 =', sigma_a0, ', on a1 =', sigma_a1, ', on a2 =', sigma_a2, ', on a3 =', sigma_a3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, instead of using the normal equations, we use singular value decomposition (SVD) to find the solution of least squares. Please read Ch. 15 of NR for more details. Remember that we have the $N \\times M$ design matrix $\\bf A$ and the vector $\\bf b$ of length $N$. We wish to mind $\\bf a$ which minimizes $\\chi^2 = |\\bf A \\cdot a - b|^2$.\n",
    "<br><br>\n",
    "Using SVD, we can decompose $\\bf A$ as the product of an $N \\times M$ column-orthogonal matrix $\\bf U$, an $M \\times M$ diagonal matrix $\\bf S$ (with positive or zero elements - the \"singular\" values), and the transpose of an $M \\times M$ orthogonal matrix $\\bf V$. ($\\bf A = USV^{T}$). <br>\n",
    "Let $\\bf U_{(i)}$ and $\\bf V_{(i)}$ denote the columns of $\\bf U$ and $\\bf V$ respectively (Note: We get $M$ number of vectors of length $M$.) $\\bf S_{(i,i)}$ are the $i$th diagonal elements (singular values) of $\\bf S$. Then, the solution of the above least squares problem can be written as:\n",
    "<br>\n",
    "$$ \\bf a = \\sum_{i=1}^M \\big( \\frac{U_{(i)} \\cdot b}{S_{(i,i)}} \\big) V_{(i)}. $$\n",
    "<br><br>\n",
    "The variance in the estimate of a parameter $a_j$ is given by:\n",
    "$$ \\sigma^2(a_j) = \\sum_{i=1}^M \\big( \\frac{V_{ji}}{S_{ii}} \\big)^2 $$\n",
    "<br>\n",
    "and the covariance:\n",
    "$$ \\mathrm{Cov}(a_j, a_k) = \\sum_{i=1}^M \\big( \\frac{V_{ji}V_{ki}}{S_{ii}^2} \\big). $$\n",
    "<br><br>\n",
    "<span style=\"color:blue\"> <i> 6. Decompose the design matrix A using SVD. Estimate the parameter $a_i$'s and its variance. </i></span><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Reference - https://docs.scipy.org/doc/numpy-1.13.0/reference/generated/numpy.linalg.svd.html\n",
    "from scipy.linalg import svd\n",
    "\n",
    "# Decompose A\n",
    "# Note: S, in this case, is a vector of length M, which contains the singular values.\n",
    "U, S, VT = svd(A, full_matrices=False)\n",
    "V = VT.T\n",
    "\n",
    "# Solve for a\n",
    "...\n",
    "a_from_SVD = ...\n",
    "\n",
    "print('Using SVD:')\n",
    "print('a0 =', a_from_SVD[0], ', a1 =', a_from_SVD[1], ', a2 =', a_from_SVD[2], ', a3 =', a_from_SVD[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Error on a\n",
    "...\n",
    "sigma_a_SVD = ...\n",
    "\n",
    "\n",
    "print('Error: on a0 =', sigma_a_SVD[0], ', on a1 =', sigma_a_SVD[1], ', on a2 =', sigma_a_SVD[2], ', on a3 =', sigma_a_SVD[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose that you are only interested in the parameters $a_0$ and $a_1$. We can plot the 2-dimensional confidence region ellipse for these parameters by building the covariance matrix:\n",
    "$$ \\mathrm{C'} =  \\binom{\\sigma({a_0})^2\\ \\ \\ \\ \\ \\ \\mathrm{Cov}({a_0, a_1})}{\\mathrm{Cov}({a_0, a_1}) \\ \\ \\ \\ \\ \\ \\sigma({a_1})^2} $$\n",
    "<br><br>\n",
    "The lengths of the ellipse axes are the square root of the eigenvalues of the covariance matrix, and we can calculate the counter-clockwise rotation of the ellipse with the rotation angle:\n",
    "$$ \\theta = \\frac{1}{2} \\mathrm{arctan}\\Big( \\frac{2\\cdot \\mathrm{Cov}({a_0, a_1})}{\\sigma({a_0})^2-\\sigma({a_1})^2} \\Big) $$\n",
    "<br>\n",
    "Then, we multiply the axis lengths by some factor depending on the confidence level we are interested in. For 68%, this scale factor is $\\sqrt{\\Delta \\chi^2} \\approx 1.52$.\n",
    "<br><br>\n",
    "<span style=\"color:blue\"> <i> 7. Compute the covariance between $a_0$ and $a_1$. Plot the 68% confidence region of the parameter $a_0$ and $a_1$. </i></span><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Compute the covariance\n",
    "...\n",
    "sigma_01 = ...\n",
    "\n",
    "# Build the covariance matrix\n",
    "CovM = np.array([[sigma_a_SVD[0], sigma_01],[sigma_01, sigma_a_SVD[1]]])\n",
    "\n",
    "from numpy.linalg import eigvals\n",
    "axis1 = 1.52*eigvals(CovM)[0]\n",
    "axis2 = 1.52*eigvals(CovM)[1]\n",
    "\n",
    "theta = np.arctan(2*sigma_01/(sigma_a_SVD[0]**2-sigma_a_SVD[1]**2))/2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Plot the 1-sigma confidence region (https://stackoverflow.com/questions/32371996/python-matplotlib-how-to-plot-ellipse)\n",
    "from matplotlib.patches import Ellipse\n",
    "import matplotlib as mpl\n",
    "\n",
    "ell = mpl.patches.Ellipse(xy=[a[0], a[1]], width=axis1, height=axis2, angle = theta*180/np.pi)\n",
    "fig, ax = plt.subplots(figsize=(7,7))\n",
    "\n",
    "ax.add_patch(ell)\n",
    "ax.set_aspect('equal')\n",
    "ax.autoscale()\n",
    "plt.grid(True)\n",
    "plt.xlabel('$a_0$')\n",
    "plt.ylabel('$a_1$')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In lecture, we discussed that we fit the existing data to obtain model parameters in data analysis, while in machine learning we use the model derived from the existing data to make prediction for new data.\n",
    "\n",
    "Next, let us take the given data and do the polynomial regression.\n",
    "\n",
    "First, split the sample into training data and the testing data. Keep 80% data as training data and uses the remaining 20% data for testing. \n",
    "\n",
    "<span style=\"color:blue\"> <i> 8. Often, the data can be ordered in a specific manner, hence shuffle the data prior to splitting it into training and testing samples. (Use https://docs.scipy.org/doc/numpy/reference/generated/numpy.random.shuffle.html) </i></span><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "...\n",
    "\n",
    "train_x = ...\n",
    "train_y = ...\n",
    "train_sigy = ...\n",
    "\n",
    "test_x = ...\n",
    "test_y = ...\n",
    "test_sigy = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many algorithms are also not scale invariant, and hence we need to scale the data (different features to a uniform scale). All this comes under preprocessing the data. http://scikit-learn.org/stable/modules/preprocessing.html#preprocessing \n",
    "We can use StandardScaler from sklearn (or write your own routine) to center the data to 0 mean and 1 variance. Note that you only center the training data and then use its mean and variance to scale the testing data before using it. \n",
    "\n",
    "In the case of polynomial regression, we need to generate polynomial features (http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PolynomialFeatures.html) for preprocessing. Note that we call each term in the polynomial as a \"feature\" in our model, and here we generate features' high-order (and interaction) terms. For example, suppose we set the degree of the polynomial to be 3. Then, the features of $X$ is transformed from $(X)$ to $(1, X, X^2, X^3)$. We can do this transform using PolynomialFeatures.fit_transform(train_x). But fit_transform() takes the numpy array of shape [n_samples, n_features]. So you need to re-define our training set as train_set_prep = train_x[:,np.newaxis] so that it has the shape [40,1].\n",
    "\n",
    "<span style=\"color:blue\"> <i> 9. Define three different polynomial models with degree of 1, 3, 10. (e.g. model = PolynomialFeatures(degree=...) ) Then, fit to data and transform it using \"fit_transform\"  </i></span><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# e.g.\n",
    "# model = PolynomialFeatures(degree = ...)\n",
    "# X_model = model.fit_transform(train_x[:,np.newaxis])\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, do the least squares linear regression. (http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html#sklearn.linear_model.LinearRegression.fit) \n",
    "\n",
    "1. define the object for linear regression: LR = linear_model.LinearRegression()\n",
    "2. Fit the linear model to the training data: LR.fit(transformed x data, y data)\n",
    "3. Define new x samples for plotting: X_sample = np.linspace(-5, 7, 100)\n",
    "4. Transform x sample: X_sample_transform = model.fit_transform(X_sample[:,np.newaxis])\n",
    "4. Predict using the linear model: Y_sample = LR.predict(X_sample_transform)\n",
    "5. Plot the fit: plt.plot(X_sample, Y_sample)\n",
    "\n",
    "\n",
    "<span style=\"color:blue\"> <i> 10. Do the linear regression for three different polynomial models defined in Part 9. Plot the fit on top of the training data (Label each curve). </i></span><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make plot\n",
    "plt.figure(figsize = (10, 7))\n",
    "\n",
    "...\n",
    "\n",
    "plt.xlabel('$x$')\n",
    "plt.ylabel('$y$')\n",
    "plt.xlim(-5, 7)\n",
    "plt.ylim(-12, 65)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<span style=\"color:blue\"> <i> 11. Plot the fit on top of the test data (Label each curve). </i></span><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make plot\n",
    "plt.figure(figsize = (10, 7))\n",
    "\n",
    "...\n",
    "\n",
    "plt.xlabel('$x$')\n",
    "plt.ylabel('$y$')\n",
    "plt.xlim(-5, 7)\n",
    "plt.ylim(-12, 65)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can obtain the estimated linear coefficients using linear_model.LinearRegression.coef_ (http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html#sklearn.linear_model.LinearRegression)\n",
    "\n",
    "\n",
    "<span style=\"color:blue\"> <i> 12. Print the linear coefficients of three polynomial models you used. For the polynomial of degree 10, do you see that high-order coefficients are very small? </i></span><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Problem 2 - Applying the PCA Method on Quasar Spectra\n",
    "\n",
    "The following analysis is based on https://arxiv.org/pdf/1208.4122.pdf.\n",
    "<br><br>\n",
    "\"Principal Component Analysis (PCA) is a powerful\n",
    "and widely used technique to analyze data\n",
    "by forming a custom set of “principal component”\n",
    "eigenvectors that are optimized to describe the\n",
    "most data variance with the fewest number of\n",
    "components. With the full set of eigenvectors the data\n",
    "may be reproduced exactly, i.e., PCA is a transformation\n",
    "which can lend insight by identifying\n",
    "which variations in a complex dataset are most\n",
    "significant and how they are correlated. Alternately,\n",
    "since the eigenvectors are optimized and\n",
    "sorted by their ability to describe variance in the\n",
    "data, PCA may be used to simplify a complex\n",
    "dataset into a few eigenvectors plus coefficients,\n",
    "under the approximation that higher-order eigenvectors\n",
    "are predominantly describing fine tuned\n",
    "noise or otherwise less important features of the\n",
    "data.\" (S. Bailey, arxiv: 1208.4122)\n",
    "<br><br>\n",
    "In this problem, we take the quasar (QSO) spectra from the Sloan Digital Sky Survey (SDSS) and apply PCA to them. Filtering for high $S/N$ in order to apply the standard PCA, we select 18 high-$S/N$ spectra of QSOs with redshift 2.0 < z < 2.1, trimmed to $1340 < \\lambda < 1620\\ \\mathring{A}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "wavelength = np.loadtxt(\"HW5_Problem2_wavelength.txt\")\n",
    "flux = np.loadtxt(\"HW5_Problem2_QSOspectra.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data dimension\n",
    "print( np.shape(wavelength) )\n",
    "print( np.shape(flux) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above cell, we load the following data: wavelength in Angstroms (\"wavelength\") and 2D array of spectra x fluxes (\"flux\").\n",
    "<br><br>\n",
    "We have 824 wavelength bins, so \"flux\" is 18 $\\times$ 824 matrix, each row containing fluxes of different QSO spectra.\n",
    "<br><br>\n",
    "<span style=\"color:blue\"> <i> 1. Plot any three QSO spectra flux as a function of wavelength. (In order to better see the features of QSO spectra, you may plot them with some offsets.) </i></span> <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Flux\" is the data matrix of order 18 $\\times$ 824. Call this matrix $\\bf X$. \n",
    "<br><br>\n",
    "We can construct the covariance matrix $\\bf C$ using the mean-centered data matrix. First, calculate the mean of each column and subtracts this from the column. Let $\\bf X_c$ denote the mean-centered data matrix.<br><br>\n",
    "$\\bf X_c$ $ =\n",
    "    \\begin{bmatrix}\n",
    "        x_{(1,1)} - \\overline{x}_1 & x_{(1,2)} - \\overline{x}_2 & \\dots  & x_{(1,824)} - \\overline{x}_{824} \\\\\n",
    "        x_{(2,1)} - \\overline{x}_1 & x_{(2,2)} - \\overline{x}_2 & \\dots  & x_{(2,824)} - \\overline{x}_{824} \\\\\n",
    "        \\vdots & \\vdots & \\vdots & \\vdots \\\\\n",
    "        x_{(18,1)} - \\overline{x}_1 & x_{(18,2)} - \\overline{x}_2 & \\dots  & x_{(18,824)} - \\overline{x}_{824}\n",
    "    \\end{bmatrix}$\n",
    "<br><br>\n",
    "where $x_{m,n}$ denote the flux of $m$th QSO in $n$th wavelength bin, and $\\overline{x}_k$ is the mean flux in $k$th wavelength bin.\n",
    "<br><br>\n",
    "Then, the covariance matrix is:\n",
    "$\\bf C$ $ = \\frac{1}{N-1}$ $\\bf X_c^T X_c.$ ($N$ is the number of QSOs.)\n",
    "<br><br>\n",
    "<span style=\"color:blue\"><i> 2. Find the covariance matrix C using the data matrix flux. </i></span><br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C = \n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\"> <i> 3. Using numpy.linalg, find eigenvalues and eigenvectors of the covariance matrix. Order the eigenvalues from largest to smallest and then plot them as a function of the number of eigenvalues. (Remember that the eigenvector with the highest eigenvalue is the principle component of the data set.)\n",
    "In this case, we find that our covariance matrix is rank-17 matrix, so we only select the first 17 highest eigenvalues and corresponding eigenvectors (other eigenvalues are close to zero). </i></span><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.linalg import eig\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make plot\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\"> <i> 4. Plot the first three eigenvectors. These eigenvectors\n",
    "represent the principal variations of the spectra with respect to that mean spectrum. </i></span><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The eigenvectors indicate the direction of the principal components, so we can re-orient the data onto the new zes by multiplying the original mean-centered data by the eigenvectors. We call the re-oriented data \"PC scores.\" (Call the PC score matrix $\\bf Z$) Suppose that we have $k$ eigenvectors. Construct the matrix of eigenvectors $\\bf V = [v_1 v_2 ... v_k]$, with $\\bf v_i$ the $i$th highest eigenvector. Then, we can get 18 $\\times\\ k$ PC score matrix by multiplying the 18 $\\times$ 824 data matrix with the 824 $\\times\\ k$ eigenvector matrix:\n",
    "<br><br>\n",
    "$$ \\bf Z = X_c V $$\n",
    "<br><br>\n",
    "Then, we can reconstruct the data by mapping it back to 824 dimensions with $\\bf V^T$:\n",
    "<br><br>\n",
    "$$ \\bf \\hat{X} = \\boldsymbol \\mu + Z V^T $$\n",
    "where $\\boldsymbol \\mu$ is the vector of mean QSO flux.\n",
    "<br><br>\n",
    "Now, comparing the original data with the reconstructed data, we can calculate the residuals. Let $\\bf X_{(i)}, \\hat{X}_{(i)}$ denote the rows of $\\bf X, \\hat{X}$ respectively. Remember that the data matrix has the dimension 18 $\\times$ 824, so each row $\\bf X_{(i)}$ corresponding the spectra of one particular QSO. (For example, if you wish to see the QSO spectra in row 7, you can plot $\\bf X_{(7)}$ as a function of wavelength.). Then, we can simply calculate the residual as $\\frac{1}{N} \\sum_{i=1}^N \\bf |\\hat{X}_{(i)} - X_{(i)}|^2$ where $N$ is the total number of QSOs (NOTE: $\\bf |\\hat{X}_{(i)} - X_{(i)}|$ is the magnitude of the difference between two vectors $\\bf \\hat{X}_{(i)}$ and $\\bf X_{(i)}$.)\n",
    "<br><br>\n",
    "<span style=\"color:blue\"> <i> 5. First, start with only mean flux value $\\boldsymbol \\mu$ (in this case $\\bf \\hat{X} = \\boldsymbol \\mu, V = 0$) and calculate the residual. Then, do the reconstruction using the first two principal eigenvectors $\\bf V = [v_1 v_2]$ and calculate the residual. Finally, let $\\bf V = [v_1 v_2 ... v_6]$ (the first six principal eigenvectors) and compute the residual. </i></span><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\"> <i> 6. For any two QSO spectra, plot the original and reconstructed spectra using the first six principal eigenvectors. </i></span><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\"> <i> 7. Plot the residual as a function of the number of included eigenvectors. </i></span><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this problem, we only have 18 QSO spectra, so the idea of using PCA may seem silly. We can also use SVD to find eigenvalues and eigenvectors. With SVD, we get $\\bf X_c = USV^T$. Then, the covariance matrix is $\\bf C$ $ = \\frac{1}{N-1}$ $\\bf X_c^T X_c$ $ = \\frac{1}{N-1}$ $\\bf VS^2V^T.$ Then, the eigenvalues are the squared singular values scaled by the factor $\\frac{1}{N-1}$ and the eigenvectors are the columns of $\\bf V$.\n",
    "<br><br>\n",
    "<span style=\"color:blue\"> <i> 8. Find the eigenvalues applying SVD to the mean-centered data matrix $\\bf X_c$. </i></span><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.linalg import svd\n",
    "\n",
    "...\n",
    "\n",
    "# Print Eigenvalues\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To Submit\n",
    "Execute the following cell to submit.\n",
    "If you make changes, execute the cell again to resubmit the final copy of the notebook, they do not get updated automatically.<br>\n",
    "__We recommend that all the above cells should be executed (their output visible) in the notebook at the time of submission.__ <br>\n",
    "Only the final submission before the deadline will be graded. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "_ = ok.submit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
