{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homework 8\n",
    "\n",
    "## <em> Linear Regression, Regularization, and Logistic & Softmax Regression</em>\n",
    "<br>\n",
    "This notebook is arranged in cells. Texts are usually written in the markdown cells, and here you can use html tags (make it bold, italic, colored, etc). You can double click on this cell to see the formatting.<br>\n",
    "<br>\n",
    "The ellipsis (...) are provided where you are expected to write your solution but feel free to change the template (not over much) in case this style is not to your taste. <br>\n",
    "<br>\n",
    "<em>Hit \"Shift-Enter\" on a code cell to evaluate it.  Double click a Markdown cell to edit. </em><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Link Okpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from client.api.notebook import Notebook\n",
    "ok = Notebook('hw8_G.ok')\n",
    "_ = ok.auth(inline = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.integrate import quad\n",
    "#For plotting\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Problem 1 - Ising Model\n",
    "\n",
    "In HW4, we did a simple ML analysis by fitting datasets generated by polynomials in the presence of noise, and this highlighted the fundamental tension common to all ML models between how well we fit the training dataset and predictions on new data. \n",
    "\n",
    "Here, we consider the problem of learning the Hamiltonian for the Ising model (https://en.wikipedia.org/wiki/Ising_model) using the linear regression. This is a lattice model proposed to explain ferromagnetism in materials. In other physics courses, you learned that elementary particles have an intrinsic property called spin, which carries magnetic moments. The magnetism of a bulk material is made up of the magnetic dipole moments of the atomic spins inside the material. The classical Ising model postulates a lattice with a spin $S$ on each site.\n",
    "\n",
    "Now consider the 1D Ising model with nearest-neighbor interactions\n",
    "\n",
    "$$H[\\boldsymbol{S}]=-J\\sum_{j=1}^L S_{j}S_{j+1}$$\n",
    "\n",
    "on a chain of length $L$ with periodic boundary conditions and $S_j=\\pm 1$ Ising spin variables. $J$ is the nearest-neighbor spin interaction\n",
    "\n",
    "With $J=1$, we draw a large number of spin configurations. We can draw them $n$ number of times: we have $n$ number of $\\boldsymbol{S}^i$, which is a vector of length $L$. Hence, $\\boldsymbol{S}$ is a matrix of $n \\times L$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\"> <i> 1. You are given 1000 random Ising states with $L=40$. (i.e. this state matrix $\\boldsymbol{S}$ should have the dimension $1000 \\times 40$, and its array elements are either 1 or -1.) Define a function which computes the energies $H$ given $\\boldsymbol{S}$. Calculate the energies of the first 10 states. </i></span> <br>\n",
    "\n",
    "Hint: Each state $\\boldsymbol{S}^i$ has its own energy, so $H[\\boldsymbol{S}]$ is a vector of length $n=1000$.\n",
    "\n",
    "We adopt the periodic boundary conditions, so when $j=L$, $j+1=1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 40)\n",
      "[[-1.  1.  1. ... -1.  1. -1.]\n",
      " [-1.  1.  1. ...  1. -1. -1.]\n",
      " [-1. -1.  1. ... -1. -1.  1.]\n",
      " ...\n",
      " [-1.  1. -1. ... -1. -1.  1.]\n",
      " [-1. -1.  1. ... -1. -1.  1.]\n",
      " [-1.  1.  1. ...  1.  1. -1.]]\n"
     ]
    }
   ],
   "source": [
    "S = np.loadtxt(\"state.txt\")\n",
    "print( np.shape(S) )\n",
    "print( S )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, suppose you do not have the knowledge of the above Hamiltonian. Instead, you are given a data set of $i=1\\ldots n$ points of the form $\\{(H[\\boldsymbol{S}^i],\\boldsymbol{S}^i)\\}$. Your task is to learn the Hamiltonian using Linear regression techniques. \n",
    "\n",
    "In the absence of any prior knowledge, one sensible choice is the all-to-all Ising model\n",
    "\n",
    "$$\n",
    "H_\\mathrm{model}[\\boldsymbol{S}^i] = - \\sum_{j=1}^L \\sum_{k=1}^L J_{j,k}S_{j}^iS_{k}^i.\n",
    "$$\n",
    "Notice that this model is uniquely defined by the non-local coupling strengths $J_{jk}$ which we want to learn. Importantly, this model is linear in ${\\mathbf J}$ which makes it possible to use linear regression.\n",
    "\n",
    "To apply linear regression, we would like to recast this model in the form\n",
    "$$\n",
    "H_\\mathrm{model}^i \\equiv \\mathbf{X}^i \\cdot \\mathbf{J},\n",
    "$$\n",
    "\n",
    "where the vectors $\\mathbf{X}^i$ represent all two-body interactions $\\{S_{j}^iS_{k}^i \\}_{j,k=1}^L$, and the index $i$ runs over the samples in the data set. To make the analogy complete, we can also represent the dot product by a single index $p = \\{j,k\\}$, i.e. $\\mathbf{X}^i \\cdot \\mathbf{J}=X^i_pJ_p$. Note that the regression model does not include the minus sign, so we expect to learn negative $J$'s."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\"> <i> 2. Create the matrix $\\mathbf{X}$. Print $\\mathbf{X}$. </i></span> <br>\n",
    "\n",
    "Hint: For each state $i$, we have the state vector $\\boldsymbol{S}^i$. $\\mathbf{X}^i$ = $\\boldsymbol{S}^i_{.T} \\otimes \\boldsymbol{S}^i_{.T}$, where $\\otimes$ is the outer product. (https://en.wikipedia.org/wiki/Outer_product)\n",
    "\n",
    "The dimension of $\\mathbf{X}^i$ is $L \\times L$. Hence, $\\mathbf{X}$ has the diemension $n \\times L \\times L$. Reshape it so that it has the dimension $n \\times L*L$ ($1000 \\times 1600$).\n",
    "\n",
    "You can either use the for-loop or use np.einsum to do the outer product (https://docs.scipy.org/doc/numpy-1.15.1/reference/generated/numpy.einsum.html). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now do the linear regression. \n",
    "$$\n",
    "H_\\mathrm{model}^i \\equiv \\mathbf{X}^i \\cdot \\mathbf{J},\n",
    "$$\n",
    "Hence, you have data ($\\mathbf{X}, H$)\n",
    "\n",
    "<span style=\"color:blue\"> <i> 3. Split the data into training and test samples. We choose that the first 70% of $n$ states are training samples, the remaining 30% test samples. No need to shuffle the data because we are already given the random set of states. Print the diemension of training and test samples.</i></span> <br>\n",
    "\n",
    "Hint: Here, H means $H[\\boldsymbol{S}]$ or $H[\\mathbf{X}]$ we calculated in Part 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In HW4, you used \"linear_model.LinearRegression()\" from scikit-learn to do the linear regression and found that using a complex model can result in overfitting. To resolve such issues, we use regularization in machine learning. A regression model that uses $L_1$ regularization technique is called Lasso Regression (https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html) and model which uses $L_2$ is called Ridge Regression (https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html).\n",
    "\n",
    "First, set up Lasso and Ridge regression models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "import seaborn\n",
    "%matplotlib inline\n",
    "\n",
    "ridge = linear_model.Ridge()\n",
    "lasso = linear_model.Lasso()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each regression model, do the following:\n",
    "\n",
    "1. Choose the regularization parameter $\\lambda$. \n",
    "2. Set the parameter using .set_params() <br>\n",
    "    e.g. lambda = 1; ridge.set_params(alpha=lambda)\n",
    "3. Fit the model<br>\n",
    "    e.g. ridge.fit(training X samples, training H samples)\n",
    "4. Compute the coefficient of determination $R^2$ of the prediction. This quantifies the performance of prediction.\n",
    "![alt text](R2.png \"Title\")\n",
    "    e.g. ridge.score(training or test X samples, training or test H samples)\n",
    "\n",
    "<span style=\"color:blue\"> <i> 4. Let lambda = np.logspace(-4, 5, 10). Compute $R^2$ score for each lambda value and plot it as a function of lambda. Do both Ridge and LASSO regression. Also, make sure to show results for both training and test samples. (4 plots) </i></span> <br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should find that the regularization parameter $\\lambda$ affects the Ridge and LASSO regressions at scales, separated by a few orders of magnitude. Therefore, it is considered good practice to always check the performance for the given model and data with $\\lambda$. \n",
    "\n",
    "At $\\lambda\\to 0$ and $\\lambda\\to\\infty$, both models overfit the data, as can be seen from the deviation of the test errors from unity (dashed lines), while the training curves stay at unity. \n",
    "\n",
    "While the Ridge regression test curves are monotonic, the LASSO test curve is not -- suggesting the optimal LASSO regularization parameter is $\\lambda\\approx 10^{-2}$. At this sweet spot, the Ising interaction weights ${\\bf J}$ contain only nearest-neighbor terms (as did the model the data was generated from).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far we have focused on learning from datasets for which there is a continuous output. Classification problems, however, are concerned with outcomes taking the form of discrete variables (i.e. categories). Here, given a spin configuration of, say, the 2D Ising model, we’d like to identify its phase (e.g. ordered/disordered).\n",
    "\n",
    "Onsager proved that this model undergoes a thermal phase transition in the thermodynamic limit from an ordered ferromagnet with all spins aligned to a disordered phase at the critical temperature $T_c/J=2/\\log(1+\\sqrt{2})\\approx 2.26$.\n",
    "\n",
    "An interesting question to ask is whether one can train a statistical model to distinguish between the two phases of the Ising model. If successful, this can be used to locate the position of the critical point in more complicated models where an exact analytical solution has so far remained elusive. \n",
    "\n",
    "In other words, given an Ising state, we would like to classify whether it belongs to the ordered or the disordered phase, without any additional information other than the spin configuration itself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To this end, we consider the 2D Ising model on a $40\\times 40$ square lattice, and use Monte-Carlo (MC) sampling to prepare $10^4$ states at every fixed temperature $T$ out of a pre-defined set. Using Onsager's criterion, we can assign a label to each state according to its phase: $0$ if the state is disordered, and $1$ if it is ordered. Our goal is to predict the phase of a sample given the spin configuration.\n",
    "\n",
    "First, load the data for the following three types of phases: ordered ($T/J<2.0$), critical ($2.0\\leq T/J\\leq 2.5)$ and disordered ($T/J>2.5$).\n",
    "\n",
    "We are given data for $T/J=1.0$, $T/J=2.25$, and $T/J=3.0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle,os\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# load data\n",
    "\n",
    "# state vector \n",
    "file_name = \"Ising2DFM_reSample_L40_T=All_labels.pkl\" \n",
    "state_vector = pickle.load(open(file_name,'rb')) \n",
    "\n",
    "# ordered phases\n",
    "file_name = \"Ising2DFM_reSample_L40_T=1.00.pkl\" \n",
    "data = pickle.load(open(file_name,'rb')) \n",
    "data = np.unpackbits(data).reshape(-1, 1600) \n",
    "data=data.astype('int')\n",
    "data[np.where(data==0)]=-1 # map 0 state to -1 (Ising variable can take values +/-1)\n",
    "\n",
    "X_ordered=data[::20]\n",
    "Y_ordered=state_vector[30000:40000][::20]\n",
    "\n",
    "# critical phases\n",
    "file_name = \"Ising2DFM_reSample_L40_T=2.25.pkl\" \n",
    "data = pickle.load(open(file_name,'rb')) \n",
    "data = np.unpackbits(data).reshape(-1, 1600) \n",
    "data=data.astype('int')\n",
    "data[np.where(data==0)]=-1 \n",
    "\n",
    "X_critical=data[::20]\n",
    "Y_critical=state_vector[80000:90000][::20]\n",
    "\n",
    "# disordered phases\n",
    "file_name = \"Ising2DFM_reSample_L40_T=3.00.pkl\" \n",
    "data = pickle.load(open(file_name,'rb')) \n",
    "data = np.unpackbits(data).reshape(-1, 1600)\n",
    "data=data.astype('int')\n",
    "data[np.where(data==0)]=-1 \n",
    "\n",
    "X_disordered=data[::20]\n",
    "Y_disordered=state_vector[110000:120000][::20]\n",
    "\n",
    "L = 40"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You have $\\textbf{X}$ for ordered, critical and disordered phases and corresponding state vector $Y$. \n",
    "For each phase (ordered, critical or disordered), we have 500 different $40\\times 40$ square lattices. So $\\textbf{X}$ has the dimension $500 \\times 40 \\times 40$. We reshape it into $500 \\times 40*40$ = $500 \\times 1600$. The state vector is a vector of length 500.\n",
    "\n",
    "Run the below cell to plot examples of typical states of the 2D Ising model for three different temperatures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot few Ising states\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "\n",
    "cmap_args=dict(cmap='plasma_r')\n",
    "\n",
    "fig, axarr = plt.subplots(nrows=1, ncols=3)\n",
    "\n",
    "axarr[0].imshow(X_ordered[100].reshape(L,L),**cmap_args)\n",
    "axarr[0].set_title('$\\\\mathrm{ordered\\\\ phase}$',fontsize=16)\n",
    "axarr[0].tick_params(labelsize=16)\n",
    "\n",
    "axarr[1].imshow(X_critical[100].reshape(L,L),**cmap_args)\n",
    "axarr[1].set_title('$\\\\mathrm{critical\\\\ region}$',fontsize=16)\n",
    "axarr[1].tick_params(labelsize=16)\n",
    "\n",
    "im=axarr[2].imshow(X_disordered[100].reshape(L,L),**cmap_args)\n",
    "axarr[2].set_title('$\\\\mathrm{disordered\\\\ phase}$',fontsize=16)\n",
    "axarr[2].tick_params(labelsize=16)\n",
    "\n",
    "fig.subplots_adjust(right=2.0)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\"> <i> 5. Combine ordered phase samples and disordered phase samples using np.concatenate. Using train_test_split (https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html), split it into training and test samples. Set train_size = 0.5. (50% of $X$ is our training samples.) Print the dimension of training and test samples. </i></span> <br>\n",
    "\n",
    "Using logistic regression, we will investigate how accurately we can distinguish between ordered and disordered phases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we compare the performance of two different optimization routines: a liblinear (the default one for scikit's logistic regression, https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html), and stochastic gradient descent (SGD, https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html). It is important to note that all these methods have built-in regularizers, and doing regularization is crucial in order to prevent overfitting.\n",
    "\n",
    "For each optimization routine, do the following:\n",
    "\n",
    "1. Choose the regularization parameter $\\lambda$. \n",
    "2. Define the logistic regressor <br>\n",
    "    e.g. $\\textbf{liblinear}$: logreg=linear_model.LogisticRegression(C=1.0/lambda,random_state=1,verbose=0,max_iter=1E3,tol=1E-5)<br> \n",
    "    e.g. $\\textbf{SGD}$: logreg_SGD = linear_model.SGDClassifier(loss='log', penalty='l2', alpha=lmbda, max_iter=100, shuffle=True, random_state=1, learning_rate='optimal')<br>\n",
    "    Use the above parameters, but you can play with them if you wish.\n",
    "3. Fit the model<br>\n",
    "    e.g. logreg.fit(training X samples, training H samples)\n",
    "4. Compute the mean accuracy on the given data.\n",
    "    e.g. logreg.score(training or test X samples, training or test H samples)\n",
    "\n",
    "<span style=\"color:blue\"> <i> 4. Let lambda = np.logspace(-5,5,11). Compute the mean accuracy for each lambda value and plot it as a function of lambda. Do both liblinear and SGD. Also, show results for both training, test samples, and critical phase samples. (6 plots) What do you find? </i></span> <br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Problem 2 - MNIST\n",
    "\n",
    "Now, we generalize logistic regression to the case of multiple categories which is called Softmax regression. A paradigmatic example of SoftMax regression is the MNIST classification problem. The goal is to find a statistical model which recognizes the ten handwritten digits. There are numerous practical applications of such a task, pretty much anywhere one can imagine dealing with large quantities of numbers.\n",
    "\n",
    "Yann LeCun and collaborators collected and processed  70000  handwritten digits to produce what became known as the most widely used database in ML, called MNIST. Here we will take a subsample of it: 3800 digits. Each handwritten digit comes in a square image, divided into a  28×28  pixel grid. Every pixel can take on  256  nuances of the gray color, interpolating between white and black, and hence each the data point assumes any value in the set  {0,1,…,255} . Since there are  10  categories in the problem, corresponding to the ten digits, this problem is a generic SoftMax regression task.\n",
    "\n",
    "Ever since, the MNIST problem has become an important standard for benchmarking the performance of more sophisticated Machine Learning models. Often times, there are contests for finding a new constellation of hyperparameters and/or model architecture which results in a better accuracy for correctly classifying the digits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.utils import check_random_state\n",
    "\n",
    "# Load MNIST data\n",
    "X = np.loadtxt(\"mnistX.dat\")\n",
    "Y = np.loadtxt(\"mnistY.dat\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"$X$\" contains information about the given MNIST digits. We have a 28x28 pixel grid, so each image is a vector of length 784; we have 3800 images (digits), so $X$ is a 3800x784 matrix. \"$Y$\" is a label (0-9; the category to which each image belongs) vector of length 3800.\n",
    "\n",
    "<span style=\"color:blue\"> <i> 1. Randomly shuffle data and split them into training and test samples using train_test_split. Let train_size = 0.8. Print the dimension of training and test samples. </i></span> <br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\"> <i> 2. Choose any five images and show what the images look like. What are the labels corresponding to them? </i></span> <br>\n",
    "\n",
    "Hint: each image is a vector of length 784. So reshape it into a 28x28 matrix.<br>\n",
    "$\\ \\ $ X_0 = X_train[0]<br>\n",
    "$\\ \\ $  X_0 = X_0.reshape((28, 28))<br>\n",
    "Then, make a plot using imshow.<br>\n",
    "$\\ \\ $  plt.imshow(X_0, cmap=plt.cm.gray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, do logistic regression in the following way:\n",
    "\n",
    "1. Scale data to have zero mean and unit variance (https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html) <br>\n",
    "  scaler = StandardScaler() <br>\n",
    "  X_train = scaler.fit_transform(X_train) <br>\n",
    "  X_test = scaler.transform(X_test) <br>\n",
    "2. Make an instance of the model using LogisticRegression (https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html). Try \"liblinear\" and \"sag\" optimization algorithms.<br>\n",
    "  $\\textbf{liblinear}$: Use solver='liblinear' and use L1 norm in the penalization (penalty='l1'). Also set C=1e5 and tol=.3<br>\n",
    "  $\\textbf{sag}$: Use solver='sag' and use L2 norm in the penalization (penalty='l2'). Also set C=1e5 and tol=.1<br>\n",
    "  e.g. model = LogisticRegression(...) <br>\n",
    "3. Train the model on the data <br>\n",
    "  e.g. model.fit(training X sample, training Y samples)<br>\n",
    "4. Predict the labels of test data.<br>\n",
    "  e.g. digit_predict = model.predict(test X samples)<br>\n",
    "5. Compute the accuracy<br>\n",
    "  e.g. model.score(test X samples, test Y samples)<br>\n",
    "  \n",
    "<span style=\"color:blue\"> <i> 3. Using both liblinear and sag solvers, compute the accuracy of the test samples. Also, measure the training time (how long it takes to train the model on the data) using time.time().  </i></span> <br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\"> <i> 4. Choose any 15 images and show what the images look like. What are the predicted labels corresponding to them? Take a look at the misclassified samples. Use the sag solver. </i></span> <br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we have 10 classes and 784 features. Using \"coef_\", we can get the coefficient of the features in the decision function. These are basically classication \"weights.\" \n",
    "\n",
    "<span style=\"color:blue\"> <i> 5. Obtain the coefficient of the features for the model using the sag solver. (coef = sag.coef_) This is a 10x784 matrix. (number of classes x number of features) Reshape it into 28x28 and make a plot for each class. How do they look? Can you recognize the digits? </i></span> <br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To Submit\n",
    "Execute the following cell to submit.\n",
    "If you make changes, execute the cell again to resubmit the final copy of the notebook, they do not get updated automatically.<br>\n",
    "__We recommend that all the above cells should be executed (their output visible) in the notebook at the time of submission.__ <br>\n",
    "Only the final submission before the deadline will be graded. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = ok.submit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
